[["index.html", "R Intro Workshop 1 Welcome to our Workshop! 1.1 Learning objectives", " R Intro Workshop The Cuddington Lab 02 July, 2024 1 Welcome to our Workshop! Please check out the Getting Ready for the Workshop activities. We would like you to download and install some files before the Workshop During the Workshop, this document will be available for you to copy code etc. After the workshop we will provide a more detailed version of this accompanying website. 1.1 Learning objectives Create graphs to visualize data and models Apply models to your own data using R programming language Interpret model fits and results Introduce principal component analyses and linear and logistic regressions Present codes and functions to conduct these analyses Identify simple and multivariate analyses and assumptions "],["get-ready-for-the-workshop.html", "2 Get ready for the workshop", " 2 Get ready for the workshop This is an introduction to basic stats in R with a focus on regression. We will assume a familiarity with R to the extent of the participant having tried to do something with the programming environment in the past (e.g., a t-test), but no more than this. 2.0.1 R from zero warmup If you have never used R in any way at a all, it may be helpful to take a quick look at this interactive tutorial for undergraduates at the University of Waterloo. Just complete the tutorial entirely online rather than downloading, starting from “R as a calculator”. https://shiny.math.uwaterloo.ca/biology/QBshiny/IntroR/ Then, when you are ready, get some help installing R on your laptop. You will need R installed to participate in workshop. 2.0.2 Download needed files and packages We will be using a flat .xlsx file of Hamilton Harbour data for our workshop exercise today. Our first job at the workshop will be to import that data. Please download the .xlxs to your personal computer prior to the workshop. In addition, we encourage you to download and install the packages we will be using for the workshop. 2.0.3 Package to read .xlsx files R is an open source platform, which means that anyone can contribute code for everyone to use. There are thousands of functions in R written by scientists, statisticians and mathematicians all over the world. Sometimes a group of functions related to a particular task are grouped together in what is called a “library” or “package”. We are going to use one such library that has been created to import .xlsx files, “readxl”. On your own computer, you will need download the package and install it. This is pretty easy. On the RStudio menu select Tools, and then Install packages from the drop down list. Enter readxl in the search window, and then select Install. You can also use the function install.packages. Just type install.packages(“readxl”). The function install.package() downloads the indicated package from the internet and stores it. Eventually, in the console window you should see an indication the R has downloaded and installed the package. Once you’ve done this once, you don’t need to do it ever again, unless you update the version of R you are using, or wish to use an updated version of the package. In addition, the exercise on multivariate analysis will use the “vegan”. Please install this package prior to the workshop as well. These packages are now part of R’s set of functions, but since it they not part of the standard environment, you will have to mention to R when you wish to use functions from this collection. The library() function tells R you wish to use commands from a particular package on this occasion. Every time you restart R, or try to compile a .Rmd file, you will have to mention to R when you wish to use functions from this collection. The simplest method to do this is to always include the statement library(readxl) at the beginning of code that uses functions from this package. Here R loads the package into active memory. library(readxl) If this command works, you are ready to go for the first part of the workshop! "],["basic-r.html", "3 Basic R 3.1 Data import &amp; packages 3.2 Data in R 3.3 Functions in R 3.4 Plotting data 3.5 The Editor window 3.6 Simple tests", " 3 Basic R 3.1 Data import &amp; packages Our first job today is to use a package to import the data we will be using throughout the workshop. If you have sucessfully installed the library readxl, we can use it to read in the .xlsx file. 3.1.1 Where’s my file?? To do the import we need the path where the file is located. There are at least three ways to find and import a file in R: use file.choose(), use the actual path to the file, use setwd() to change the current directory to the location of the file 3.1.1.1 Try it now: Import the data file using the function read_excel() from the readxl library, and indicate the path using one of the methods above. # load the library library(readxl) # import the xlsx file ham &lt;- read_excel(&quot;ind dates flat all parameters Hamilton only FOR Kim.xlsx&quot;) 3.1.2 Did the import work?? Once you have read in the data, it is critically import to check that th import worked properly. Problems that can arise from incorrectly formatted files can then be resolved. Problems can include: numeric data being read in as character data, column names being read in as the the first row of data, missing values being coded as something other than NA. There are a number of commands to look at a portion of the dataset. Four quick methods to check data import: print or view(), colnames()**, head() or tail(), str() # View(ham) colnames(ham[, 1:5]) [1] &quot;waterbody&quot; &quot;area_group&quot; &quot;Latitude&quot; &quot;Longtitude&quot; [5] &quot;Station_Acronym&quot; tail(ham, n = 5) # A tibble: 5 × 109 waterbody area_group Latitude Longtitude Station_Acronym report_Stn &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Hamilton Harbour west 43.3 -79.9 HHCarolsP CP 2 Hamilton Harbour west 43.3 -79.9 HHRHYC RHYC 3 Hamilton Harbour west 43.3 -79.9 HHRHYC out RHYC out 4 Hamilton Harbour wind 43.3 -79.8 HH4_PHYTO WSTP 5 Hamilton Harbour west 43.3 -79.9 HHBayfront-West BFW # … with 103 more variables: SamplingDate &lt;dttm&gt;, season &lt;dbl&gt;, year &lt;dbl&gt;, # Julian_Day &lt;dbl&gt;, Julian_Week &lt;dbl&gt;, Month &lt;dbl&gt;, Station_depth &lt;dbl&gt;, # `water level` &lt;dbl&gt;, Ammonia_ECCC1m &lt;dbl&gt;, DIC_ECCC1m &lt;dbl&gt;, DOC_ECCC1m &lt;dbl&gt;, # POC_ECCC1m &lt;dbl&gt;, Chl_ECCC1m &lt;dbl&gt;, `Chl Cor_ECCC1m` &lt;dbl&gt;, NO2_NO3_ECCC1m &lt;dbl&gt;, # PON_ECCC1m &lt;dbl&gt;, `TKN dissolved_ECCC1m` &lt;dbl&gt;, SRP_ECCC1m &lt;dbl&gt;, TP_ECCC1m &lt;dbl&gt;, # `TP dissolved_ECCC1m` &lt;dbl&gt;, Chl_a_uncorrected &lt;dbl&gt;, Secchi &lt;dbl&gt;, # `Kd light_attenuation` &lt;dbl&gt;, Cyano &lt;dbl&gt;, Chloro &lt;dbl&gt;, Eugleno &lt;dbl&gt;, … str(ham, list.len = 5) tibble [742 × 109] (S3: tbl_df/tbl/data.frame) $ waterbody : chr [1:742] &quot;Hamilton Harbour&quot; &quot;Hamilton Harbour&quot; &quot;Hamilton Harbour&quot; &quot;Hamilton Harbour&quot; ... $ area_group : chr [1:742] &quot;deep&quot; &quot;NE&quot; &quot;deep&quot; &quot;west&quot; ... $ Latitude : num [1:742] 43.3 43.3 43.3 43.3 43.3 ... $ Longtitude : num [1:742] -79.9 -79.8 -79.8 -79.9 -79.9 ... $ Station_Acronym : chr [1:742] &quot;HH908&quot; &quot;HH6&quot; &quot;HH258&quot; &quot;HHBayfront&quot; ... [list output truncated] 3.2 Data in R You’ll notice that the data are structured in columns. This is a dataframe, one of the most used data structures in R. Data structures are sets of variables organized in a particular way. In R there are 4 primary data structures we will use repeatedly: vectors, matrices, dataframes and lists. 3.2.1 Selecting portions of a dataframe To start our examination and analysis of this data, we need to be able to select items of interest. Dataframes are indexed by rows and columns. If you want the item from the 5th row and 2nd column type mydata[5,2]. If you need just one column you can type either mydata[,2], which grabs everything in column 2, or if you know the name of the column mydata$Population. To get rows 2 to 5 you can enter mydata[2:5,2]. You can also combine this with the column name which may be easier to read, as mydata$Population[2:5]. 3.2.1.1 Try it now: Select just the first 5 rows and first 3 columns of the dataframe you read into R. # A tibble: 5 × 3 waterbody area_group Latitude &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 Hamilton Harbour deep 43.3 2 Hamilton Harbour NE 43.3 3 Hamilton Harbour deep 43.3 4 Hamilton Harbour west 43.3 5 Hamilton Harbour deep 43.3 We can create new data subsets by using the c() or combine function. 3.2.1.2 Try it now: Let’s access the first 3 and column 10, and save the result to a new variable ’ ham_sub’ # save a subsection of a dataframe ham_sub = ham[1:5, c(1:3, 9:10)] ham_sub # A tibble: 5 × 5 waterbody area_group Latitude year Julian_Day &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Hamilton Harbour deep 43.3 2016 131 2 Hamilton Harbour NE 43.3 2021 174 3 Hamilton Harbour deep 43.3 2021 174 4 Hamilton Harbour west 43.3 2022 174 5 Hamilton Harbour deep 43.3 2022 174 We can also just grab a single column, using the column name to identify it as mydata$thiscolumn or mydata[,“thiscolumn”]. 3.2.1.3 Try it now: Access the Station_Acronym column of the data [1] &quot;HH908&quot; &quot;HH6&quot; &quot;HH258&quot; &quot;HHBayfront&quot; &quot;HH908&quot; &quot;HH9031&quot; 3.2.1.4 Try it now: Our subsetted data is not ordered by year or group, so let’s use the order() function to rearrange # save a subsection of a dataframe ham_sub = ham_sub[order(ham_sub$area_group, ham_sub$year), ] ham_sub # A tibble: 5 × 5 waterbody area_group Latitude year Julian_Day &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Hamilton Harbour deep 43.3 2016 131 2 Hamilton Harbour deep 43.3 2021 174 3 Hamilton Harbour deep 43.3 2022 174 4 Hamilton Harbour NE 43.3 2021 174 5 Hamilton Harbour west 43.3 2022 174 3.2.2 Using conditional statements to access data We can also use conditional statements to access portions of the data. Conditional statements evaluate to TRUE or FALSE. The “==” symbol is used to determine if a variable is equal to some value, while “!=” evaluates is something is not equal. As you might expect we can also use greater than (“&gt;”) or less than (“&lt;”) conditionals. 3.2.2.1 Try it now: Enter ham$calanoid[ham$Station_Acronym==“HH6”] and examine the data. [1] 84.094200542 13.196863744 64.145098250 58.389201770 25.256348256 0.006374018 Of course, this gives us all of the sampling dates for this station. Let’s assume we only want data from 2016. In that case we can combine our conditional statements using “&amp;” for AND and “|” for OR. 3.2.2.2 Try it now: Use an “&amp;” symbol to indicate that we want the selected data to be from station “HH6” and to be from the 2016. [1] 64.145098 25.256348 70.721192 142.447448 89.268307 70.066000 32.332505 [8] 78.560037 7.974158 120.229185 91.171246 19.286071 104.393878 If we wanted we could use the same conditionals to grab the julian sampling day for these observations, and save the result to new vectors cal_HH6_2016 = ham$calanoid[ham$Station_Acronym == &quot;HH6&quot; &amp; ham$year == 2016] jul_cal_HH6_2016 = ham$Julian_Day[ham$Station_Acronym == &quot;HH6&quot; &amp; ham$year == 2016] Table 3.1: Calanoid data for 2016 at station HH6 calanoid julian 64.145098 145 25.256348 131 70.721192 160 142.447448 188 89.268306 201 70.066000 215 32.332505 174 78.560037 258 7.974158 228 120.229185 242 91.171246 270 19.286071 286 104.393878 299 3.2.2.3 Try it now: Access the calanoid data at station “HH6” for 2014 [1] 3.7525745 4.4195122 4.4719882 49.3636318 7.1764905 0.8381301 3.7054878 [8] 6.7292412 4.0907797 5.7956002 1.0151220 5.4204413 3.3 Functions in R Next, let’s figure out how to complete some simple calculations with these values, like finding the mean and standard deviation. We have already used some functions in R. R contains thousands of functions, and more are being added everyday. If you are trying to find a function that does something you need to do, you can use the command apropos(“keyword”) to see if there is a function that contains the keyword in its name. 3.3.0.1 Try it now: For example if you wanted to find a function that calculated means you might: type ‘apropos(“mean”)’. What happens? One of the listed functions “mean” looks promising. Your next step might be to get more information by typing ‘help(mean)’ or ‘example(mean)’. 3.3.0.2 Try it now: You should see that the funciton mean() works on a vector of numeric data. Perhaps the function does not contain the keyword in its name, in this case try ??”keyword phrase” to get R to search function descriptions If the internal help menu lets you down you can also try a search at http://www.rseek.org/ 3.3.0.3 Try it now: Use these tools to find a function that calculates standard deviation Then mean this function and a function to calculate means to get the mean and standard deviation of calanoid density in 2014. [1] 8.064917 [1] 13.14886 3.3.1 Summarizing data Other functions can help with quick data summaries. for example, the table() function can be used to take a quick look at the number of sampling dates for each station. 3.3.1.1 Try it now: Use this function on the Station_Acronym column of the data Station Name No. of samples CCIW dock 3 HH17 13 HH1B 22 HH2 1 HH2000 1 HH2001 1 HH2002 1 HH2003 1 HH2004 1 HH258 172 HH2B 21 HH39 1 HH4_PHYTO 22 HH6 136 HH8 38 HH9031 22 HH9033 22 HH908 96 HH917 22 HHBayfront 22 HHBayfront-West 22 HHBFouter 3 HHBURSTP 22 HHCarolsP 22 HHRHYC 21 HHRHYC out 21 HHWC 13 The summary() function can also give information about a vector. #### Try it now: Use the summary function on the “Chl_a_uncorrected” data for site HH258 Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 3.060 9.508 12.600 15.004 19.253 70.020 25 3.4 Plotting data A quick check of data can also be done with simple plots. for example, let’s see what the Chl_a_uncorrected data from 2016 looks like using the plot() function. plot(y = ham$Chl_a_uncorrected, x = ham$Julian_Day) Of course, this is the data from every station. Let’s just plot station HH258, and colour code by sampling year sub2_ham = ham[ham$Station_Acronym == &quot;HH258&quot;, ] plot(y = sub2_ham$Chl_a_uncorrected, x = sub2_ham$Julian_Day, col = sub2_ham$year) Kind of hard to read… I think I wan to group the data by decade, and let’s pimp the plot a little. We also need axis labels, a nicer symbol and a legend. 3.5 The Editor window Preveiously you could have completed all of these commands in the Console window where R would execute them immediately. Now we’re going to have a bunch of commands, some of which we may want to tweek (e.g., colours). This time, let’s save our code in the Editor window so that it is easy to change and rerun. We’ll create a new file using the drop down menus. Click File then New File, and then R Script. A blank document will appear in the Editor window is found in the top left of the default R Studio. 3.5.0.1 Try it now: Create a new file, enter the following code, and then save it using the drop down menu or cntrl-s or command-s. Then run the code Notice that I’ve used a factor variable to control the colour of the data from each decade. Let’s try one more. Let’s quickly visualize the number of calanoid samples from each area by date. Again we will use a factor variable to control appearance ham$area_group = as.factor(ham$area_group) plot(calanoid ~ SamplingDate, data = ham, col = area_group) legend(&quot;topleft&quot;, legend = levels(ham$area_group), bty = &quot;n&quot;, pch = 1, col = c(1:length(levels(ham$area_group)))) This is too cluttered, so I am going to summarize the data by year and area, and then plot that. I’m going to plot the data from each area individually starting with the first one, and then adding subsequent areas to the same plot using the lines() function. samps = as.data.frame(table(ham[, c(&quot;year&quot;, &quot;area_group&quot;)])) samps$year = as.numeric(as.character(samps$year)) plot(Freq ~ year, data = samps[samps$area_group == &quot;deep&quot;, ], bty = &quot;L&quot;, xlim = c(min(samps$year), max(samps$year)), ylim = c(min(samps$Freq), max(samps$Freq)), col = &quot;black&quot;, type = &quot;b&quot;, main = &quot;Samples per year by area group&quot;) lines(Freq ~ year, data = samps[samps$area_group == &quot;NE&quot;, ], col = &quot;red&quot;, type = &quot;b&quot;) lines(Freq ~ year, data = samps[samps$area_group == &quot;west&quot;, ], col = &quot;blue&quot;, type = &quot;b&quot;) lines(Freq ~ year, data = samps[samps$area_group == &quot;wind&quot;, ], col = &quot;green&quot;, type = &quot;b&quot;) legend(&quot;topleft&quot;, legend = levels(ham$area_group), lty = 1, bty = &quot;n&quot;, pch = 1, col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)) At this point, the plotting is getting a bit more complex, so you may want to use another library called *ggplot2** to do some of your plotting. This library of functions has a non-intuitive command structure, but is quick for plotting multiple sets of data on the same plot with a legend. # similar plot using ggplot library(ggplot2) ggplot(samps, aes(x = year, y = Freq, group = area_group, colour = area_group)) + geom_line() 3.6 Simple tests Finally, let’s complete a couple of simple tests. year_sub = ham[(ham$year == 2019), ] t.test(calanoid ~ area_group, data = year_sub[(year_sub$area_group == &quot;deep&quot; | year_sub$area_group == &quot;NE&quot;), ]) Welch Two Sample t-test data: calanoid by area_group t = -1.5146, df = 15.19, p-value = 0.1504 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -19.198594 3.237734 sample estimates: mean in group deep mean in group NE 8.47007 16.45050 boxplot(calanoid ~ area_group, data = year_sub) "],["linear-regression.html", "4 Linear regression 4.1 What does this output tell us? 4.2 What are the assumptions of a linear regression? 4.3 How do you evaluate the assumptions of a linear regression? 4.4 How can we plot this linear regression? 4.5 References and resources 4.6 Alternative resource:", " 4 Linear regression 4.0.1 Introduction A linear model is used when you want to test a prediction of correlation between an independent (or predictor) variable and a dependent (or response) variable (Phillips, 2018). For example, for our Hamilton Harbour data, we will be testing for a correlation between water level (our independent variable) and total zooplankton (our dependent variable) counted in the net grab. 4.0.2 Data import &amp; packages # A tibble: 0 × 109 # … with 109 variables: waterbody &lt;chr&gt;, area_group &lt;chr&gt;, Latitude &lt;dbl&gt;, # Longtitude &lt;dbl&gt;, Station_Acronym &lt;chr&gt;, report_Stn &lt;chr&gt;, SamplingDate &lt;dttm&gt;, # season &lt;dbl&gt;, year &lt;dbl&gt;, Julian_Day &lt;dbl&gt;, Julian_Week &lt;dbl&gt;, Month &lt;dbl&gt;, # Station_depth &lt;dbl&gt;, water level &lt;dbl&gt;, Ammonia_ECCC1m &lt;dbl&gt;, DIC_ECCC1m &lt;dbl&gt;, # DOC_ECCC1m &lt;dbl&gt;, POC_ECCC1m &lt;dbl&gt;, Chl_ECCC1m &lt;dbl&gt;, Chl Cor_ECCC1m &lt;dbl&gt;, # NO2_NO3_ECCC1m &lt;dbl&gt;, PON_ECCC1m &lt;dbl&gt;, TKN dissolved_ECCC1m &lt;dbl&gt;, # SRP_ECCC1m &lt;dbl&gt;, TP_ECCC1m &lt;dbl&gt;, TP dissolved_ECCC1m &lt;dbl&gt;, … 4.0.3 Linear regression using water level and total zooplankton Call: lm(formula = waterlevel ~ total.zoop, data = ham) Residuals: Min 1Q Median 3Q Max -0.75082 -0.18579 -0.02821 0.14399 0.98481 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.487e+01 1.659e-02 4511.728 &lt; 2e-16 *** total.zoop 2.537e-04 3.189e-05 7.954 1.08e-14 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2986 on 537 degrees of freedom (203 observations deleted due to missingness) Multiple R-squared: 0.1054, Adjusted R-squared: 0.1037 F-statistic: 63.26 on 1 and 537 DF, p-value: 1.078e-14 4.1 What does this output tell us? The call in the summary output displays the call that was given to R, our linear regression coding using the lm() function. The residuals show the difference between the observed versus the expected values for the linear regression analysis. Under coefficients, the estimate shows the average change in value of our dependent variable (total zooplankton) when there is a 1 unit increase in the independent variable (water level). The standard error is a value that represents any uncertainty with the estimate value, where the t value shows the difference between these two values. The p-value is arguably the most important in this output. It indicates the level of significance of the relationship between the independent (water level) and dependent (total zooplankton) variables. Aka whether the value of the independent variable significantly impacts the value of the dependent variable. A p-value smaller than 0.05 is generally considered to represent a statistically significant relationship, whereas a p-value larger than 0.05 is generally considered to represent a statistically insignificant relationship between the independent and dependent variable. 4.1.1 What does the summary output tell us about the relationship between water level and total zooplankton? 4.2 What are the assumptions of a linear regression? There is a linear relationship between the independent and dependent variable (Statistics solutions, 2024). The residuals are normally distributed (Statistics solutions, 2024). The variance of the residuals should be consistent across all the values of the independent variable (Statistics solutions, 2024). Each observation is independent of one another (there is no significant correlation between the independent variable values) (Statistics solutions, 2024). 4.3 How do you evaluate the assumptions of a linear regression? You can evaluate the assumptions of a linear regression by creating diagnostic plots. These look at the residuals of the linear regression, that were shown using the summary() function above, to determine if the linear model is appropriate for the data (the data does not violate any assumptions) (Statistics solutions, 2024). This will create 4 diagnostic plots: residuals versus leverage, scale-location, Q-Q residuals and residuals versus fitted values. 4.3.1 Let’s try… 4.3.2 1. The residuals versus fitted plot determines if the residuals exhibit non-linear behaviour (Statistics solutions, 2024). If the red line roughly follows the horizontal line on the plot, the residuals are showing linear behaviour (Statistics solutions, 2024). In this case, the red line follows the horizontal line very closely. We can determine then that the linear model is suitable for this dataset. 4.3.3 2. The Q-Q plot evaluates our second assumption; the residuals are normally distributed (Statistics solutions, 2024). If the points on the plot follow the diagonal line, then the residuals are normally distributed (Statistics solutions, 2024). In this case, the points fall roughly on the diagonal line, with some variation towards the upper end. 4.3.4 3. The Scale-Location plot is used to determine our third assumption; the variance of the residuals should be consistent across all the values The the independent variable (Statistics solutions, 2024). This is also called homoscedasticity (Bobbitt, 2021). In this case, the line deviates slightly from horizontal, so there may be some slight variance of residuals. 4.3.5 4. The Residuals versus Leverage plot displays any influential points in the dataset compared to Cook’s distance (Bobbitt, 2021). Ideally, the points should fall close to the ‘0’ line on the plot with little variation. Any points that fall past the line indicating Cook’s distance are considered strongly influential points (Bobbitt, 2021). In this case, we have one point that falls past the line of Cook’s distance (point 282), indicating that we have one strongly influential point in our dataset. 4.4 How can we plot this linear regression? In this case, water level does not appear to have a large impact on the amount of zooplankton collected. The regression, shown by the red line on the plot, shows a horizontal pattern between water level (m) and total zooplankton collection. 4.5 References and resources Assumptions of multiple linear regression analysis. Statistics Solutions. (2024, April 17). https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/ Bobbitt, Z. (2021, July 23). How to interpret diagnostic plots in R. Statology. https://www.statology.org/diagnostic-plots-in-r/ Phillips, N. D. (2018, January 22). Yarrr! The Pirate’s Guide to R. YaRrr! The Pirate’s Guide to R. https://bookdown.org/ndphillips/YaRrr/ * Check out chapter 15 on linear regression analysis 4.6 Alternative resource: Another way to evaluate the assumptions of a linear regression is to compare fitted values (theoretical values of the dependent variable that are predicted by the model, assuming linearity) and the observed values of the dependent variable in a plot (Phillips, 2018). If the model fitted values forms a diagonal line with the observed values of the dependent variable, then the linear model fits the data well, and the assumptions of the linear model are met (Phillips, 2018). See Phillips resource for more details and sample coding. "],["multiple-linear-regression-model.html", "5 Multiple Linear Regression Model 5.1 Introduction 5.2 Data preparation 5.3 Model building 5.4 Model output 5.5 What could go wrong? 5.6 References and Resources", " 5 Multiple Linear Regression Model 5.1 Introduction 5.1.1 What is a Mulitple Linear Regression Model A multiple linear regression model allows using multiple predictor variables to predict a single response variable, as in this equation: \\[\\hat{y}=\\hat{a}x_1+\\hat{b}x_2+\\hat{c}x_3...+\\hat{z}\\] It’s basically an extension of the previously introduced single-variate linear regression model, both mathematically speaking and in the sense of building them in R. 5.1.2 Recipe for multi-variate model in R To build and interpret a multi-variate model: First, we will start from preparing data for building multi-variate model. We will select and check our variables of interests in Data preparation. The characteristics of our data might affect the subsequent model building and evaluation process. Once we prepared the data, we can start to build the actual model. The standard R package stats provides the simplest way to do single and multi-variate linear regression model through the function lm(). We will introduce the detailed usage of this function for multi-variate linear regression model in Model building. At the end, we examine our model and analyze the presented relationships between the predictors and the response. An entire section (Model output) will be dedicated in introducing ways to interpret the model output and to evaluate the model based on all the assumptions multi-variate linear regression model made. 5.1.2.1 Loading data: Let’s load all the necessary packages before proceeding to the actual modeling process. For the following sections, there are only two necessary packages readxl and car. If you have not installed these package before, please install it using install.package(). For example: install.package(readxl) Note: I hide the warnings from this code chunk because loading packages give out a lot of warnings. If you want to see those warnings, delete warning=FALSE. For this and following sections, we will be using 3 predictor variables (water level, dissolved inorganic carbon amount and bottom hypoxia) and 1 response variable (total zooplankton) from the Hamilton Harbour AOC project dataset. So, we need to read the dataset and separate our selected variables from the complete dataset first: 5.2 Data preparation First, we want to determine the characteristics of the three predictor variables we have, most importantly, are they continuous or categorical? Continuous variable: data that are measured and ordered. It can be any specific value within a certain numerical range. For example: 74.34 74.36 74.4 74.42 74.49 74.5 74.54 74.55 74.56 74.59 74.6 74.63 74.64 74.65 2 0 0 2 0 4 6 3 4 3 6 0 0 4 74.7 74.71 74.74 74.75 74.78 74.79 74.81 74.84 74.85 74.86 74.87 74.88 74.89 74.9 8 4 4 0 10 10 7 11 5 6 0 4 0 4 74.91 74.92 74.94 74.95 74.96 74.98 74.99 75.02 75.03 75.04 75.05 75.06 75.08 75.09 6 2 0 12 4 4 0 8 4 0 4 4 1 12 75.1 75.11 75.12 75.13 75.14 75.16 75.17 75.18 75.19 75.21 75.22 75.23 75.24 75.3 2 0 7 0 5 6 4 2 8 0 0 0 2 4 75.33 75.35 75.43 75.53 75.69 75.7 75.8 75.81 75.91 0 0 1 2 2 0 4 0 0 Categorical variable: data that is divided into categories with distinct label. These data can’t be ordered or measured as continuous variables. For example: n y 116 101 5.2.1 Check data type Because of the intrinsic differences between continuous and categorical variables, we always want to check whether our data is in the correct data type before proceeding to the next step. Sometimes the data is not read in as the correct data type, and other times data types got altered during previous data modification procedures. We can check data type easily using the class() function, for example: [1] &quot;factor&quot; Oops, it seems we do need to modify the data type of our dataset. Let’s continue to check the other three columns of our dataframe using the same code: 5.2.1.1 Set data type: We can set the data type back into numeric for our continuous variables using as.numeric(). For our categorical variables, we can transform them into groups/factors using as.factor(). I also transformed them into binary dummy variables that are required for some functions to accommodate categorical variables. Note: As you noticed, the as.xxx() format is used to transform data type directly in R. You can explore some other options on your own, such as as.character(), as.integer()…. 5.3 Model building Continuous variables work smoothly in multi-variate regression model. For categorical variable data, lm() function has embedded an automatic dummy coding process that will transform categorical values into n-1 groups of 0s and 1s. (You can check contrast() function to learn more about the dummy coding process.) Generating the model itself is similar to the single variable model and is very simple once we prepared our data. Simply adding a + sign between predictor variables to include more than one variable in your model. So, instead of lm(response ~ predictor, data = dataframe), we are now using lm(response ~ predictor 1 + predictor 2 ..., data = dataframe) 5.3.0.1 Building the actual model: Let’s try to include the two additional predictor variables DIC and bottomhypoxia in our model based on the previous single variate model: 5.4 Model output 5.4.1 Model assumptions Just like the simple linear regression model, the multi-variate linear regression models come with similar assumptions: The predictor variables and the response variable have a linear relationship since we are doing a linear regression model. The model also assumed normal distribution in residuals. Homoscedasticity that there exists equal variance in residuals. And the multi-variate feature brings in one more assumption: No multicollinearity. The independent variables should be independent of each other. 5.4.1.1 Plot diagnosis plots: I found it much easier to build the model first then check the assumptions as we can utilize these auto-generated model diagnosis plots from the lm() function. These plots can be called by simply run plot(model_name). Let’s try this: You should see four diagnosis plots generated: Residuals vs Fitted plot, Normal Q-Q plot, Scale-Location plot, and The Residuals vs Leverage plot. 5.4.2 Evaluate model assumptions Residuals vs Fitted plot: Residuals vs Fitted plot checks linearity assumption. If the red line, represents residual distribution, in the plot align with the dashed black line (horizontal at zero), it indicates linear relationship exists in the model. we can also test linear relationship by simply do a scatter plot between each predictor variable and the response variable, using plot(). For example, plot(multi_data$totalzoop~multi_data$DIC), and we can add a linear regression line by using abline() and a single linear model between these two variables using lm(). The R squared of the linear regression line can be extracted and evaluate whether the relationship is linear: `summary(lm(totalzoop~DIC,data=multi_data))$r.squared`. Normal Q-Q plot: Normal Q-Q plot checks the normality of residuals assumption. If the standardized residuals follow the straight dashed line, then the assumption is fulfilled. Another simpler way to visualize and check it is to plot histogram using (hist()) model residuals. Scale-Location plot: Scale-Location plot is used to check the homoscedasticity. If the residuals have equal variance, the standardized will spread randomly with the red line approximate horizontal line. If heteroscedasticity is found, we can try to eliminate it by doing non-linear data transformation or adding quadratic terms. Residuals vs Leverage plot: The Residuals vs Leverage plot is not used to check any of the model assumptions. However, it is still important and worth checking as it is used to identify extreme values that can cause a huge influence to our analysis of the model output based on Cook’s distance. Extreme values can be identified as any of the values are outside of the dashed lines. The above diagnosis plots check three out of the total four assumptions of this model, the last one, Multicollinearity, can be checked by the following several ways depending on the data characteristics: If we only have continuous variables in our dataset, we can check this using correlation matrix through cor() or/and corrplot(), or variance inflation factor using vif() from the car package, orvifcor() and vifstep() from the usdm package. If we have both continuous and categorical variables in our dataset, we need to transfer our categorical variables into dummy variables and viewing them as a single entity. We have already done that in [2. Data preparation]. The vif() function will automatically detect our categorical variables and compute us the generalized vifs. Our variables look good. If multicollinearity is found (vif &gt; 5 is indicating problematic amount of multicollinearity), we can try to remove it by identify and remove the predictor variable that is causing troubles. 5.4.3 Output interpretation To access model output, the easiest way is to call the summary() method just like the single variate linear regression model: However, unlike the single variate model, you can compare the coefficients and significance of multiple predictors and decide on whether you want to exclude some of the variables in your final model. In this case, DIC seems to be the most significant variable and we might be able to get a similarly accurate model from this single predictor 5.5 What could go wrong? Check carefully whether the values in your data set are in their correct data types, especially for the categorical variables if you have any. Be careful with the comma , and tilde ~ symbols when you are dealing with functions in R. They represent opposite positions of the independent and dependent variables. Always check the model assumptions to see if linear regression model is indeed the appropriate model to use. 5.6 References and Resources Other resources on introductions to multivariate linear regression model in R: https://www.datacamp.com/tutorial/multiple-linear-regression-r-tutorial https://bookdown.org/jimr1603/Intermediate_R_-_R_for_Survey_Analysis/regression-model.html#multiple-linear-regression https://library.virginia.edu/data/articles/getting-started-with-multivariate-multiple-regression https://rpubs.com/bensonsyd/385183 If you would like to have prettier plots for presentation purposes, check the ggplot2 package: https://ggplot2.tidyverse.org/ "],["logistic-regression.html", "6 Logistic regression 6.1 What is a logistic regression? 6.2 How are logistic regressions different from linear regressions? 6.3 How does this type of data look like? 6.4 What does the logistic curve mean? 6.5 Mathematical representation 6.6 What kind of predictors can we have in a logistic regression? 6.7 Key binomial logistic model assumptions 6.8 Aquatic ecology studies using logistic regressions 6.9 Creating a logistic model in R 6.10 What could go wrong? 6.11 R resources", " 6 Logistic regression 6.1 What is a logistic regression? Association between binary response variable and predictors Example: species distribution models Binary response variable: presence (1) or absence (0) of species in area Predictions of habitat suitability 6.2 How are logistic regressions different from linear regressions? Linear regression - Quantitative predictions: straight line - Correlation and test for significance of regression - Model comparison (eg., single versus several predictors) Logistic regression - We can also do all of that! - Main difference: our outcomes are now binary - Examples of binary outcomes: presence versus absence; positive to a disease versus negative; dead versus alive - Outcomes can be categorized as 1 (e.g., success) and 0 (e.g., failure) 6.3 How does this type of data look like? Hamilton Harbour dataset: let’s suppose that it becomes hard for zooplankton to eat when there are more “less edible” algae than “edible” algae in the environment That was the criteria for populating the column “Easy to eat”: Nitrate/nitrite Edible Less edible Easy to eat 2.23 884.60 1645.7 no 2.50 900.00 1133.3 no 2.50 923.20 939.9 no 2.13 1546.40 1312.0 yes 2.13 811.90 454.1 yes 2.62 339.16 308.2 yes 2.67 1376.80 552.2 yes When we plot this type of binary data, we see that observations are either of the two outcome possibilities: This type of data is best fit by an s-shaped curve instead of a line. And this is another difference between linear and logistic regressions. 6.4 What does the logistic curve mean? It represents the probability of positive outcomes depending on predictors As we move along the curve and our predictor values change, we go from 0 to 100% probability of our outcome 6.5 Mathematical representation Logistic regression: transformation of response variable to get linear relationship logit (log of probability of success/probability of failure) Explanatory variable is in log(odds) Equation for logistic regression: \\[ Log (p/1-p) = b0+b1*x1+e \\] Log (p/1-p): response variable b0: y intercept x: predictor variable b1: slope for explanatory variable 1 e: error If we had more explanatory variables, we could keep adding them to the equation above as b2*x2 and so forth. 6.6 What kind of predictors can we have in a logistic regression? Just like in a linear regression, we can use continuous and/or discrete variables to make predictions. Here are some examples: Continuous variables Temperature Precipitation Water depth Nitrogen concentration Discrete variables Levels of aquatic vegetation Soil type Water body type 6.7 Key binomial logistic model assumptions Dependent variable has 2 levels or is a proportion of successes Observations are independent Normal distribution of data or residuals is not needed 6.8 Aquatic ecology studies using logistic regressions The distribution of gammarid species was predicted using logistic regressions, where current velocity was the most important factor explaining their distribution (Peeters &amp; Gardeniers, 1998) Foraging shift behaviour from the benthos to the water surface in brown trout (1 = surface prey consumed, 0 = no surface prey consumed) was predicted using fish length as a predictor in a logistic regression (Sánchez-Hernández &amp; Cobo, 2017) Amphipod Gammaridae; Brown Trout, USFWS Mountain-Prairie 6.9 Creating a logistic model in R 6.9.1 Steps to run a logistic model in R Select potentially interesting predictors Format predictors to correspond to binomial levels Select time period and location Run model Interpret model Plot results 6.9.2 Let’s create this first model together First, we are going to have a look at some potentially interesting variables: knitr::kable(ham[c(25:30), c(2, 8, 26, 47, 98)], row.names = FALSE, digits = 2, align = rep(&quot;c&quot;, 3), col.names = c(&quot;Location&quot;, &quot;Season&quot;, &quot;Total phosphorus&quot;, &quot;Filamentous diatom biomass&quot;, &quot;Epilimnion temperat.&quot;)) Location Season Total phosphorus Filamentous diatom biomass Epilimnion temperat. deep 2 0.01 0.00 21.79 deep 2 0.01 3.10 21.83 NE 1 0.02 NA 19.89 deep 1 0.02 NA 19.71 NE 1 NA NA 13.90 deep 1 NA 287.18 16.05 Let’s consider that filamentous diatom is our response variable of interest, as this food source is hard for zooplankton to consume. We will look at epilimnion temperature as a potential explanatory variable. First, let’s create a copy of the original dataset so that we can introduce modifications but keep the original data in case we need it. filam_diatom &lt;- ham Before we can get started with the analysis, we need to remove NA data. We will now do that for the potential response variable filamentous diatom column, and for the potential explanatory variable epilimnion temperature: filam_diatom &lt;- filam_diatom[!is.na(filam_diatom$filamentous_Diatom), ] filam_diatom &lt;- filam_diatom[!is.na(filam_diatom$mean_mixing_depth_temp), ] 6.9.2.1 Try it now Now you can do this last step (removing NA data) for total phosphorus (“TP dissolved_ECCC1m”), as we will consider this as another potential explanatory variable later on. # Use the original dataset &#39;ham&#39; to subset this data, and # give this new dataset a name, like &#39;filam_diatom_P&#39; # Don&#39;t forget to remove NA data from both the response and # the explanatory variables Let’s go back to our “filam_diatom” dataset. Now we will create a new column to describe presence or absence of filamentous diatoms. Ensure the reference group (“absent”) is the first to be shown: filam_diatom$filam_presence &lt;- ifelse(filam_diatom$filamentous_Diatom &gt; 0, &quot;present&quot;, &quot;absent&quot;) filam_diatom$filam_presence &lt;- factor(filam_diatom$filam_presence) levels(filam_diatom$filam_presence) [1] &quot;absent&quot; &quot;present&quot; Subset to analyse at specific times of the year and at specific locations Here I selected summer conditions. Ideally we should also subset by station (column “Station_Acronym”), but because there’s not enough data to do that, let’s subset by depth, by removing records in deep locations: filam_diatom &lt;- subset(filam_diatom, (season == 2 | season == 3) &amp; (!area_group == &quot;deep&quot;)) Run model using glm function and family binomial model &lt;- glm(filam_presence ~ mean_mixing_depth_temp, data = filam_diatom, family = binomial) Now we can check the model results summary(model) Call: glm(formula = filam_presence ~ mean_mixing_depth_temp, family = binomial, data = filam_diatom) Deviance Residuals: Min 1Q Median 3Q Max -1.9998 -0.8972 0.5144 0.8733 1.2380 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 10.1581 3.8402 2.645 0.00816 ** mean_mixing_depth_temp -0.4202 0.1736 -2.420 0.01551 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 79.807 on 69 degrees of freedom Residual deviance: 72.195 on 68 degrees of freedom AIC: 76.195 Number of Fisher Scoring iterations: 5 So we can see that our p-value for the epilimnion temperature predictor is smaller than 0.05, which means that the presence of filamentous diatoms can be predicted by temperatures. Let’s see what each component of the model result summary means: We are ready for the best part: plotting model predictions The dotted curves are confidence intervals, which show us the range in which we are 95% sure about the location of true values, based on our data # Now, let&#39;s calculate predicted probabilities for different # values of mean_mixing_depth_temp # Create a sequence of values for mean_mixing_depth_temp mean_mixing_depth_temp_values &lt;- seq(min(filam_diatom$mean_mixing_depth_temp), max(filam_diatom$mean_mixing_depth_temp), length.out = length(filam_diatom$mean_mixing_depth_temp)) # Create a data frame with mean_mixing_depth_temp values newdata &lt;- data.frame(mean_mixing_depth_temp = mean_mixing_depth_temp_values) # Predict probabilities for each value of # mean_mixing_depth_temp predicted_probabilities_filam &lt;- predict(model, newdata = newdata, type = &quot;response&quot;) # Calculate confidence intervals manually z &lt;- qnorm(1 - (1 - 0.95)/2) # 95% confidence level se &lt;- sqrt(predicted_probabilities_filam * (1 - predicted_probabilities_filam)/nrow(filam_diatom)) lower_bound &lt;- predicted_probabilities_filam - z * se upper_bound &lt;- predicted_probabilities_filam + z * se # Convert &#39;present&#39; and &#39;absent&#39; to 1 and 0 presence_numeric_filam &lt;- ifelse(filam_diatom$filam_presence == &quot;present&quot;, 1, 0) # Plot the predicted probabilities with confidence intervals plot(mean_mixing_depth_temp_values, predicted_probabilities_filam, type = &quot;l&quot;, main = &quot;Filamentous diatom presence&quot;, xlab = &quot;Epilimnion temperature (°C)&quot;, ylab = &quot;Predicted probability&quot;, cex.axis = 1.5, ylim = c(0, 1), lwd = 2) lines(mean_mixing_depth_temp_values, lower_bound, col = &quot;blue&quot;, lty = 2) lines(mean_mixing_depth_temp_values, upper_bound, col = &quot;blue&quot;, lty = 2) points(filam_diatom$mean_mixing_depth_temp, presence_numeric_filam) 6.9.2.2 Try it now 6.9.3 Creating your logistic model Now it’s your turn! Run this next logistic model using total phosphorus as a predictor, and filamentous diatoms as the response variable again. Formatting Do you remember how you previously labeled your dataset for the phosphorus analysis? You can use that for this exercise. You have already removed NA data, but before proceeding, let’s relabel the response variable to remove empty spaces, which may cause errors going forward: names(filam_diatom_P)[names(filam_diatom_P) == &quot;TP dissolved_ECCC1m&quot;] &lt;- &quot;Total_phosphorus&quot; Now you can create a new column that describes the presence or the absence of filamentous diatoms across the dataset: # To create a new column, first type the label you chose # previously for the dataframe related to the phosphorus # analysis (something like &#39;filam_diatom_P&#39;), then create a # column name, and use the function &#39;ifelse&#39; to label all # observations where measurements were greater than zero as # &#39;present&#39;, and all observations where measurements were # equal to zero as &#39;absent&#39;: your_dataset_name$new_column_name &lt;- ifelse(your_dataset_name$filamentous_Diatom &gt; 0, &quot;present&quot;, &quot;absent&quot;) Format this new column as a factor # use the factor function here # and check how it looks by calling: levels(your_dataset_name$filamentous_Diatom) Select time periods and locations You can choose to select summer, or another time period. Because we have more data for phosphorus, you can select a single station (column 5: “Station_Acronym”) # Use the subset function to select desired periods of time # and locations. You can use &#39;&amp;&#39; for more than one selection # at the same time, and &#39;|&#39; for selecting either one or other # option: your_dataset_name &lt;- subset(your_dataset_name, (season == &quot;your selection&quot; | season == &quot;your selection&quot;) &amp; (Station_Acronym == &quot;your selection&quot;)) Run model # Use glm function and family binomial your_model_name &lt;- glm(&quot;response variable&quot; ~ &quot;explanatory variable&quot;, data = your_dataset_name, family = binomial) Check model results using the summary function summary(your_model_name) How well did this predictor do? Plot model predictions # Create a sequence of values for Total_phosphorus Total_phosphorus_values &lt;- seq(min(filam_diatom_P$Total_phosphorus), max(filam_diatom_P$Total_phosphorus), length.out = length(filam_diatom_P$Total_phosphorus)) # Create a data frame with Total_phosphorus values newdata &lt;- data.frame(Total_phosphorus = Total_phosphorus_values) # Predict probabilities for each value of Total_phosphorus predicted_probabilities_P &lt;- predict(model, newdata = newdata, type = &quot;response&quot;) # Calculate confidence intervals manually z &lt;- qnorm(1 - (1 - 0.95)/2) # 95% confidence level se &lt;- sqrt(predicted_probabilities_P * (1 - predicted_probabilities_P)/nrow(filam_diatom)) lower_bound &lt;- predicted_probabilities_P - z * se upper_bound &lt;- predicted_probabilities_P + z * se # Convert &#39;present&#39; and &#39;absent&#39; to 1 and 0 presence_numeric_P &lt;- ifelse(filam_diatom_P$filam_presence == &quot;present&quot;, 1, 0) # Plot the predicted probabilities with confidence intervals plot(Total_phosphorus_values, predicted_probabilities_P, type = &quot;l&quot;, main = &quot;Filamentous diatom presence&quot;, xlab = &quot;Total phosphorous (dissolved fraction) (mg/L)&quot;, ylab = &quot;Predicted probability&quot;, cex.axis = 1.5, ylim = c(0, 1), lwd = 2) lines(Total_phosphorus_values, lower_bound, col = &quot;blue&quot;, lty = 2) lines(Total_phosphorus_values, upper_bound, col = &quot;blue&quot;, lty = 2) points(filam_diatom_P$Total_phosphorus, presence_numeric_P) 6.10 What could go wrong? Mixing up predictors and response variables in the logistic model equation If you get a warning when running your logistic model, you may have mixed up the predictors and the response variables. The response variable should be the first one to appear after the opening parenthesis In addition to the variable mix-up, this warning tells us that we may be using quantitative measures as our response variable, which is not appropriate for a logistic regression. See how this error looks like: model &lt;- glm(Total_phosphorus ~ filam_presence, data = filam_diatom_P, family = binomial) Warning in eval(family$initialize): non-integer #successes in a binomial glm! Incorrectly coding data so that it is not independent Imagine you have counts of living and dead organisms in your dataset: example_independence &lt;- data.frame(date = rep(&quot;Jan-1-2024&quot;, times = 3), location = c(&quot;station-1&quot;, &quot;station-2&quot;, &quot;station-3&quot;), living_daphnia = floor(runif(3, min = 0, max = 101)), dead_daphnia = floor(runif(3, min = 0, max = 101))) # runif function generates random numbers within the range # established by &#39;min&#39; and &#39;max&#39; values knitr::kable(head(example_independence)) date location living_daphnia dead_daphnia Jan-1-2024 station-1 20 53 Jan-1-2024 station-2 36 2 Jan-1-2024 station-3 55 100 In this case, instead of considering each individual as “living” or “dead”, you should calculate the proportion of living organisms per replicate like this: example_independence$proportion &lt;- round(example_independence$living_daphnia/(example_independence$living_daphnia + example_independence$dead_daphnia), 2) # the round function ensures we have 2 decimals in our # proportion values knitr::kable(head(example_independence)) date location living_daphnia dead_daphnia proportion Jan-1-2024 station-1 20 53 0.27 Jan-1-2024 station-2 36 2 0.95 Jan-1-2024 station-3 55 100 0.35 This proportion will be your response variable for the logistic model. When using proportions, you should also provide the “weights” information in the glm formula (i.e., a dataset with total number of trials per replicate, or the sum of events where we got success + events where we got failure). Categorical response variable is formatted as character instead of factor Formatting data as factors allows for defining order or reference group for statistical testing # Here we are selecting and formatting the binomial data as # we did before, but now we accidentally forget to format the # new column as &#39;factor&#39; example &lt;- ham example$filam_presence &lt;- ifelse(example$filamentous_Diatom &gt; 0, &quot;present&quot;, &quot;absent&quot;) model &lt;- glm(filam_presence ~ mean_mixing_depth_temp, data = example, family = binomial) Error in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1 Check the data in the problematic column: str(example$filam_presence) chr [1:742] &quot;present&quot; NA NA &quot;present&quot; NA NA NA NA NA NA NA NA &quot;present&quot; NA ... See how we need to remove NA data and format the column as factor for our model to run nicely: example &lt;- example[!is.na(example$filamentous_Diatom), ] example$filam_presence &lt;- factor(example$filam_presence) levels(example$filam_presence) [1] &quot;absent&quot; &quot;present&quot; model &lt;- glm(filam_presence ~ mean_mixing_depth_temp, data = example, family = binomial) No more errors now! That’s it for now, but if you are interested in more complex logistic models, here are some resources: 6.11 R resources Multiple logistic regression Building Skills in Quantitative Biology (Cuddington, Edwards, &amp; Ingalls, 2022) Getting started with Multivariate Multiple Regression (Ford, 2024) Multinomial logistic regression Multinomial logistic regression (Russell, 2022) Mixed-effects logistic regression Mixed-Effects Binomial Logistic Regression (Schweinberger, 2022 ) Mixed-effects logistic regression (Sonderegger, Wagner, &amp; Torreira, 2018) "],["multivariate-analysis.html", "7 Multivariate analysis 7.1 Libraries and imports 7.2 Introduction to multivariate analysis 7.3 Principle Component Analysis (PCA) 7.4 Non-metric multidimensional scaling (nMDS) 7.5 Other resources", " 7 Multivariate analysis 7.1 Libraries and imports 7.2 Introduction to multivariate analysis In previous sections, we have discussed scenarios where there is one response variable. If we have multiple responses, \\(y_1\\)…\\(y_n\\), and multiple predictors, \\(x_1\\)…\\(x_n\\), then we need multivariate approaches. These methods allow us to represent the variables or observations in a lower-dimensional space, such as a two-dimensional or three-dimensional plot, while preserving the overall structure of the data. OUTLINE: “Large zooplankton such as Daphnia, large copepods or predatory Cladocera (Bythotrephes, Cercopagis, Leptodora) are much better prey for forage fishes, so changes in their populations (or shifting drivers) are of particular interest.” Question: What are the major drivers of Diaphnia biomass? Variables: Daphnia biomass (mg/m3), water column temperature (°C), epilimnion temperature(°C), particulate organic nitrogen (mg/L), dissolved inorganic carbon (mg/L), particulate organic carbon (mg/L) In our new dataframe, different seasons are represented by numeric numbers from 1 to 5. We would like to re-code them into string factors, for easier visualization in later graphics. There are many ways to do this, here we introduce using the factor() function. 1 2 3 4 57 113 118 40 7.3 Principle Component Analysis (PCA) Principle component analysis is a linear transformation method that converts the original set of variables into a new set of linearly uncorrelated variables, called principal components (PCs), which are sorted in decreasing order of variance. 7.3.1 Correlation examination First of all, we need to examine the correlation between our variables. We can achieve this by running a correlation test using cor() function, or creating a correlation plot using pairs() function. daphnia column.temp epili.temp pon dic poc daphnia 1.0000000 0.2293285 0.3230253 0.1233690 -0.1169047 0.1437126 column.temp 0.2293285 1.0000000 0.8167008 0.3083830 -0.4815168 0.3670961 epili.temp 0.3230253 0.8167008 1.0000000 0.3098929 -0.5021353 0.3857810 pon 0.1233690 0.3083830 0.3098929 1.0000000 -0.2834075 0.9459712 dic -0.1169047 -0.4815168 -0.5021353 -0.2834075 1.0000000 -0.3846432 poc 0.1437126 0.3670961 0.3857810 0.9459712 -0.3846432 1.0000000 Dimension reduction techniques such PCA works the best when variables are strongly correlated with each other. From the above correlation test output and plot, we can see that some variables clearly have a linear relationship, such as water column temperature and epilimnion temperature, or water column temperature and dissolved inorganic carbon. 7.3.2 PCA with standardized data Now we can start with running our principle component analyses. PCA can be computed using various functions in R, such as prcomp() in stats package, princomp() in stats package, rda() in vegan package. Here we demonstrate using the vegan package, since it also allows easy visualization of our results. Keep in mind that we need to run the PCA on all columns containing continuous variables, which is column 4 - 9 in the ham.multi dataset. Call: rda(X = ham.multi[, 4:9], scale = TRUE) Partitioning of correlations: Inertia Proportion Total 6 1 Unconstrained 6 1 Eigenvalues, and their contribution to the correlations Importance of components: PC1 PC2 PC3 PC4 PC5 PC6 Eigenvalue 3.0133 1.2857 0.9019 0.57514 0.1770 0.047034 Proportion Explained 0.5022 0.2143 0.1503 0.09586 0.0295 0.007839 Cumulative Proportion 0.5022 0.7165 0.8668 0.96266 0.9922 1.000000 Scaling 2 for species and site scores * Species are scaled proportional to eigenvalues * Sites are unscaled: weighted dispersion equal on all dimensions * General scaling constant of scores: 6.655409 Species scores PC1 PC2 PC3 PC4 PC5 PC6 daphnia 0.9767 -0.9315 -2.2964 0.52564 0.10599 0.00045 column.temp 2.1337 -1.1389 0.4750 -0.84259 0.77265 0.01578 epili.temp 2.1945 -1.1988 0.1987 -0.62965 -0.83207 -0.03190 pon 1.9948 1.7653 -0.3022 -0.17437 0.04365 -0.40417 dic -1.8082 0.5322 -0.9965 -1.68304 -0.04658 0.04264 poc 2.1620 1.5780 -0.1877 -0.01352 -0.04505 0.42518 Site scores (weighted sums of species scores) PC1 PC2 PC3 PC4 PC5 PC6 sit1 -0.601634 0.3885262 -0.158695 0.102933 0.1655011 0.333793 sit2 -0.196510 -0.2004236 0.127205 -0.642207 0.1070714 0.007366 sit3 -0.190506 -0.2084349 0.120883 -0.637570 -0.2916448 -0.038325 sit4 -0.146629 -0.2634170 0.156631 -0.736766 0.2421800 0.011811 sit5 -0.188428 -0.2099248 0.139186 -0.663422 0.2032287 0.016335 sit6 -0.159409 -0.2524372 0.098350 -0.674051 -0.1976415 -0.034287 sit7 -0.076292 -0.4923387 -0.460920 0.135292 -0.1504689 -0.032391 sit8 -0.242282 -0.3421360 -0.091195 0.087947 0.0485281 0.106174 sit9 -0.357991 0.0798586 -0.230681 -0.044937 -0.4058041 0.186120 sit10 -0.050404 -0.3665789 0.142980 -0.146548 0.2774292 0.201427 sit11 -0.150478 -0.2847522 -0.309505 0.274005 -0.4738068 0.140669 sit12 -0.337010 0.1455180 -0.454757 0.071055 0.2362431 0.299432 sit13 -0.337780 0.1634913 -0.316337 0.009996 -0.3430906 0.234404 sit14 0.085235 0.1125541 -0.119853 0.345385 -0.1086747 -0.239454 sit15 0.169030 -0.3284519 0.126677 0.059390 0.2607800 0.256351 sit16 0.074023 -0.2105352 0.046216 0.264210 -0.4355833 0.199042 sit17 -0.363011 -0.1765170 -0.346708 0.067022 0.2769241 0.125014 sit18 -0.562023 0.3145768 -0.115591 0.144698 0.1579823 0.082778 sit19 -0.622300 0.3896560 -0.157199 0.257194 0.1967906 0.099963 sit20 0.295754 -0.5256298 0.301202 0.050542 0.4036026 0.286246 sit21 0.080335 -0.2343257 0.329830 0.391710 -0.9413234 0.181483 sit22 0.064431 -0.0771622 0.116601 -0.857415 0.2200680 -0.459718 sit23 -0.675883 0.3923273 -0.018497 0.449871 0.1192565 0.304262 sit24 -0.135983 -0.4384811 0.074982 -0.005865 -0.3415944 0.069614 sit25 0.088093 -0.3148339 0.069515 -0.461382 0.2670816 0.213447 sit26 -0.053809 -0.1692026 -0.311115 -0.017424 -0.5091618 0.158389 sit27 -0.189221 -0.3551163 -0.265061 -0.746650 0.2761418 0.144720 sit28 -0.213056 -0.4895306 -1.712512 0.106851 -0.2507470 0.097229 sit29 -0.301556 0.2632200 -0.144557 -0.006038 -0.0450036 0.107109 sit30 -0.116025 -0.2366661 0.136455 -0.646192 0.0046395 -0.306293 sit31 -0.233191 -0.0849504 0.094738 -0.430542 -0.6580469 -0.355354 sit32 -0.188465 -0.4950414 -0.767565 0.179809 0.4138109 0.079078 sit33 -0.344512 -0.2353391 -0.320127 0.182031 -0.2537983 0.035538 sit34 -0.552933 0.5635551 -0.087426 0.214043 0.0929099 -0.002595 sit35 0.070856 -0.0371205 0.296809 -0.215109 0.1911239 0.651012 sit36 -0.050585 0.1118484 0.181867 0.048057 -0.4879175 0.601342 sit37 -0.001368 -0.3748648 0.341673 -0.380526 0.1499189 0.403845 sit38 -0.164824 -0.1808435 0.129301 0.008164 -0.8597551 0.326569 sit39 -0.002258 -0.4468546 -0.345700 0.204673 -0.4797931 0.046607 sit40 -0.658107 0.4103807 -0.038934 0.413499 0.1233154 0.299319 sit41 -0.685332 0.4459234 -0.043973 0.457343 0.1142028 0.304051 sit42 -0.636464 0.3844606 -0.014735 0.367512 0.1270427 0.295078 sit43 -0.635698 0.3755266 -0.083176 0.404023 0.1418420 0.296862 sit44 -0.051700 -0.2422172 0.062483 -0.088798 -0.7895507 -0.110036 sit45 -0.117382 -0.5288729 -1.346803 0.369430 0.2067918 -0.012208 sit46 0.069726 -0.3343328 0.150197 -0.184734 0.2222523 0.056053 sit47 -0.009283 -0.3053495 -0.517768 0.323157 -0.4962005 -0.004552 sit48 0.324297 -0.8011352 -1.025878 -0.159805 0.6611809 -0.404563 sit49 -0.572683 -0.1601513 0.077303 -0.294912 0.1746057 0.294308 sit50 -0.670952 -0.0366263 0.012642 -0.101994 -0.2063745 0.272779 sit51 0.140785 -0.4387497 -0.146614 -0.002810 0.1138704 0.372380 sit52 -0.002446 -0.2377218 -0.061840 0.183943 -0.6404200 0.318043 sit53 0.180688 -0.4321212 0.218720 -0.075534 -1.0367754 0.203217 sit54 -0.017454 -0.0848075 0.109711 -0.725129 0.0136644 -0.377140 sit55 -0.209531 0.1479993 -0.092916 -0.303361 -0.8223595 -0.429077 sit56 0.179798 -0.3595944 0.386314 -0.099935 -0.0696554 0.519738 sit57 0.052419 -0.2121843 0.190606 0.215235 -0.6784735 0.479495 sit58 0.020016 0.0119456 0.121663 -0.294417 -0.9570242 -0.967955 sit59 0.400659 -0.1616583 0.457942 -0.437373 0.3014286 0.301572 sit60 0.560513 -0.3662953 -0.055354 0.337368 0.5355596 0.967432 sit61 0.319248 -0.0116154 0.226912 0.574044 -0.7195058 0.877030 sit62 0.467245 -0.6145573 -0.309669 0.510230 0.4872574 -0.437835 sit63 -0.005537 -0.4046332 0.076429 0.045836 -0.4032174 0.309377 sit64 0.353855 -0.5659524 0.399966 0.055834 0.3825439 0.246008 sit65 0.210952 -0.3935707 0.238846 0.380903 -0.4566147 0.183200 sit66 -0.311418 0.2304439 -0.019041 -0.304726 -0.0295281 0.198389 sit67 -0.248268 0.1525857 0.036017 -0.436714 0.2352427 0.214389 sit68 -0.023758 -0.0441231 0.110941 -0.585451 0.2002636 0.467699 sit69 0.300587 -0.1402726 0.301549 -0.234052 0.3612497 0.226666 sit70 0.260766 -0.0871755 0.298804 -0.163906 0.0097139 0.195860 sit71 0.328518 -0.1597144 0.453931 -0.360160 0.3436743 0.218174 sit72 0.305429 -0.1323185 0.426782 -0.311879 0.3995596 0.229379 sit73 0.343408 -0.1807773 0.442334 -0.375700 0.3304913 0.213623 sit74 -0.329572 0.2220566 -0.263851 -0.243631 -0.1602358 -0.481056 sit75 -0.169016 0.0135890 -0.218972 -0.519677 0.2502275 -0.469256 sit76 0.340054 -0.1704422 0.399061 -0.361336 0.2965710 0.148741 sit77 0.179256 0.0227403 0.211968 0.005914 -0.5788880 0.085761 sit78 0.015099 -0.0610295 0.217940 0.029484 -0.2684907 0.392647 sit79 -0.634790 0.3843066 -0.046588 0.226109 0.1237562 0.227684 sit80 -0.513706 0.2305775 0.013352 0.010535 0.1533495 0.205257 sit81 0.113534 -0.4746931 -0.494225 0.069246 -0.5528284 -0.153651 sit82 0.305218 -0.7040281 -0.262234 -0.374713 0.5375353 -0.073401 sit83 -0.615141 0.5785679 -0.148650 0.101003 0.1013078 0.228286 sit84 -0.602897 0.5631233 -0.141870 0.079081 0.0933601 0.224795 sit85 0.131766 -0.5080465 -0.679223 -0.068195 0.2482970 -0.273091 sit86 0.110387 -0.5374517 -1.191430 0.267994 -0.5193733 -0.351977 sit87 0.333081 -0.5659301 -0.697441 0.330753 -0.0044578 -0.037000 sit88 0.140781 -0.2825730 -0.461190 0.503618 -0.6301462 -0.067330 sit89 -0.577166 0.5092729 -0.137689 -0.064587 0.1260932 -0.110930 sit90 -0.655848 0.6080611 -0.186450 0.081307 0.0927379 -0.097888 sit91 -0.254287 0.3581156 0.077888 0.245248 -0.0980810 0.169985 sit92 -0.148178 -0.3494305 -0.036452 -0.015904 0.1359261 0.088213 sit93 -0.227284 -0.2425419 -0.028524 0.114014 -0.4839503 0.035705 sit94 -0.040705 0.1278095 0.022138 -0.543950 -0.8604326 -0.554602 sit95 0.099763 -0.0516397 0.094250 -0.816380 -0.0025181 -0.488807 sit96 0.453914 0.0137287 0.531979 0.159589 0.3456826 0.268526 sit97 0.350973 0.1395898 0.429069 0.388943 -0.3572459 0.212250 sit98 0.127513 -0.0003175 -0.013372 -0.737579 -0.6707923 0.600394 sit99 0.265750 -0.1831185 0.004573 -0.977522 0.2249344 0.671108 sit100 0.279212 -0.2094760 -0.070967 -0.952614 0.1024863 0.654950 sit101 0.211370 -0.1537485 -0.376488 -0.666508 -0.4732464 0.606346 sit102 0.193000 -0.0900151 -0.031676 -0.836466 -0.2419151 0.634516 sit103 -0.297400 -0.2621026 -0.497503 0.398365 -0.0901018 0.063194 sit104 0.357127 -0.4005323 0.529665 -0.071394 0.4502265 0.529480 sit105 0.221640 -0.2388356 0.355001 0.260687 -0.8064763 0.418617 sit106 0.183880 -0.6396421 -0.540960 0.591605 0.5891042 0.217798 sit107 0.006836 -0.3346557 0.056621 0.543657 -0.1490942 0.170157 sit108 0.227157 -0.4074716 -0.017855 0.542259 -0.4240453 -0.613907 sit109 0.393985 -0.6442697 -0.138526 0.335134 0.5250249 -0.542662 sit110 -0.650837 0.4864998 -0.087815 0.263479 0.1106049 0.144238 sit111 -0.622823 0.4511662 -0.071831 0.212259 0.1241192 0.139783 sit112 -0.223825 0.2610579 -0.353120 -0.439564 0.2275944 -0.378404 sit113 -0.196459 0.1850378 -0.699920 -0.283846 0.0694338 -0.400299 sit114 -0.213973 0.2841360 -0.043314 -0.620302 -0.0165593 -0.409034 sit115 -0.206596 0.2516754 -0.238879 -0.524513 0.0556324 -0.401693 sit116 -0.190791 0.1807635 -0.669441 -0.313333 0.2290644 -0.383825 sit117 -0.220877 0.0881044 -0.557498 -0.189230 0.2443573 -0.536259 sit118 -0.322428 0.2568827 -0.269279 -0.184472 -0.2557208 -0.571896 sit119 -0.179526 -0.1115506 0.118881 -0.758779 0.0424682 0.253470 sit120 -0.275614 0.0038383 0.008200 -0.542628 -0.3749775 0.227616 sit121 -0.064420 -0.2906288 0.288757 -0.132895 0.3236620 -0.095299 sit122 -0.099320 -0.2880363 -0.095495 0.142161 -0.0386207 -0.126694 sit123 -0.345881 0.0267871 0.087892 0.370685 0.2507271 -0.202062 sit124 -0.317963 0.1454291 0.087554 0.152535 -0.0425345 -0.498102 sit125 -0.355994 0.1863634 0.002472 0.261401 -0.2226646 -0.509819 sit126 0.087541 0.0206278 -0.081689 -0.869218 0.1731138 -0.350093 sit127 -0.069965 0.2111700 -0.251276 -0.520981 -0.5310645 -0.394744 sit128 0.198997 -0.1087755 0.350935 -0.195022 -0.6569742 -0.141445 sit129 0.352769 -0.2994905 0.480920 -0.523953 0.3741039 -0.059391 sit130 0.264606 -0.3548128 0.536761 0.002581 0.2799021 -0.377423 sit131 0.141690 -0.2054824 0.408690 0.274064 -0.3522607 -0.421499 sit132 0.032817 0.2127734 0.031416 -0.228944 -0.5604923 0.039918 sit133 0.180750 0.0317612 0.176270 -0.553760 0.3277889 0.107216 sit134 0.327261 -0.2499365 0.272084 -0.369005 -0.0001001 0.442435 sit135 0.333459 -0.1625336 -0.175234 -0.864377 0.0683503 0.627579 sit136 0.265048 -0.1555385 0.313957 -0.217517 0.0302980 0.178865 sit137 0.092998 0.0613670 0.201953 0.126967 -0.9277546 0.108720 sit138 0.275135 -0.2499460 -0.096815 0.155587 -0.5146280 -0.560207 sit139 -0.177522 0.2004884 0.022587 -0.178982 -0.1240356 -0.294904 sit140 0.356127 -0.0560102 0.140922 -0.249920 0.0977009 0.300497 sit141 0.219452 0.1052487 -0.048493 0.087489 -0.9758759 0.210365 sit142 -0.004242 0.4831121 -0.050579 -0.407648 0.1463115 -0.037647 sit143 0.144270 0.1336023 0.162683 -0.257287 0.0169087 -0.605615 sit144 0.062439 0.2278850 0.027630 -0.039841 -0.7598186 -0.674421 sit145 0.551809 -0.2095484 0.315975 -0.128892 0.3128877 0.363357 sit146 0.374868 0.0221235 0.277409 0.179291 -0.5340477 0.306313 sit147 0.115710 -0.2969842 -0.118930 0.141867 -0.5122830 -0.225172 sit148 -0.661456 0.3659404 -0.049195 0.325794 0.1343395 -0.227002 sit149 0.112627 -0.1081745 0.158430 0.141772 0.1840558 -0.287189 sit150 0.198518 0.0902042 0.392609 0.353746 -0.8436704 -0.157154 sit151 0.015529 -0.1430778 0.059869 -0.495271 0.3281069 0.372957 sit152 -0.132934 0.0479138 -0.004224 -0.215078 -0.5390903 0.307774 sit153 0.358682 -0.1834412 0.125579 -0.145788 0.0112285 -0.532675 sit154 0.215053 -0.3698726 -0.130456 -0.164603 -0.1817236 0.456732 sit155 0.251258 -0.3563529 0.410915 -0.533037 0.3422083 0.505198 sit156 0.254591 -0.3990229 0.076364 -0.347176 0.1477349 0.484257 sit157 0.169557 -0.2863488 0.067004 -0.200004 -0.4232813 0.438518 sit158 0.236084 -0.3410533 0.361620 -0.470117 -0.1610148 0.452496 sit159 0.083032 -0.5880063 -0.275885 -0.405768 0.4415620 -0.215163 sit160 0.016807 -0.5830356 -1.005474 0.117945 -0.3060020 -0.281437 sit161 0.206758 -0.8757492 -1.343968 -0.004703 0.7183357 -0.205725 sit162 0.035831 -0.5171925 -0.208833 -0.365144 0.1607681 -0.236831 sit163 -0.026802 -0.4357151 -0.228781 -0.249621 -0.2546052 -0.269883 sit164 0.015151 0.3255025 0.008009 -0.548357 0.0970408 -0.111379 sit165 -0.123475 0.4949913 -0.120157 -0.263189 -0.1444925 -0.108597 sit166 -0.080067 -0.1185539 0.260965 -0.269746 0.1570498 -0.199903 [ reached getOption(&quot;max.print&quot;) -- omitted 162 rows ] After completing the dimension reduction, each sample now appears as a point in space specified by its new position along the principle component axes. There coordinates are referred to as “site scores” in rda() results, and we can assess such information with the scores() function. Meanwhile, our original variables are projected on the the new principle components. They are defined as “Loadings” and are referred to as “species scores” in rda() results. This information can be obtained with the scores() function too. Remember: “Sites” refer to your observations (the rows in your dataset). “Species” refer to your descriptors (the columns in your dataset), which are the different environmental variables. 7.3.3 Screeplot Now, let’s determine how many principle components to retain for further analysis. The screeplot() function allows us to visualize the variance explained by each of the principle components. Ideally, a curve should be steep, and then bend at an “elbow”, after which the curve flattens out. The first few principle components usually account for a large portion of the variance in the data, and should be retained. Meanwhile, we can also look at the proportional variance explained by each principle component. Such information is available in the summary() of our PCA results. We see the first two PCs together explain roughly 72% of the total variance in this dataset. Along with the screeplot, we are confident that the first two PC are sufficient enough to represent our data. For easier usage in future, let’s store them in a list: [1] 50.22 21.43 7.3.4 Plot ordination for PCA After we have chosen the first two principle components, now let’s start visualizing our multidimensional data in a 2-dimensional space. As we have seen in previous materials, there are many different methods/packages for creating plots in R. Some of these resources are listed in the “Other Resources” section at the end if you are interested. For this tutorial, we demonstrate using the vegan package to visualize our multivariate results. In general, plotting ordination with vegan follows two steps: Use ordiplot() to create an empty canvas (You can specify the title, axes, limits, and many other features during this step). Use points() to add points representing samples or variables in the new dimensions. Use text() to add labels. Plot 1 - Site Plot (samples) we can visualize the positions of our samples on the new axes (PC1 and PC2). This is the “site plot”. What if we are interested in the seasonal patterns of our samples on the reduced dimensions? We can use the same graphing techniques, but group our dataset using different seasons. Make sure to label your seasons in the “legend”. There are many other functions in the vegan package that allows customization of your ordination plots. For example, ordiellipse(pca.ham, display=\"sites\", conf=0.95, kind=\"sd\", groups=ham.multi$season) function allows us to add 95% confidence intervals to each group of samples. Plot 2 - Species Plot (variables) We can also plot how much influence each variable has on each of the two principle components. This is the “species plot”. The steps are very similar. Instead of display = \"sites\", we specify display = \"species\" in points(). Remember that positively correlated variables are grouped close together (formed angle around 0 degree); variables with about a 90 degree angle are not correlated; negatively correlated variables are positioned on opposite sides of the plot origin (~180 degree angle). The distance between the variables and the origin measure the contribution of that variable to the ordination. A shorter arrow indicates its less importance for the ordination. A longer arrow means the variable is better represented. Plot 3 - BiPlot (samples and variables) Finally, we can visualize all above information (sites and species) on one graph with a biplot. This time, we use the biplot() function in base R to do this. Some important points for the biplot() function: biplot() allows different scaling options to preserve either the Euclidean distance (in multidimensional space) among objects (sites), or the correlations/covariances between variables (species). To learn more, please refer to the function description or these online tutorials: QCBS workshop - Unconstrained Ordination Scaling; Tutorial 14.2 - Principle Component Analysis (PCA). biplot() can also be used to show only the samples or variables, by setting display = \"sites\" or display = \"species\". However, it does not allow us to visualize samples in different groups. This is why we introduce the slightly more complicated ordiplot() method in the previous sections. 7.4 Non-metric multidimensional scaling (nMDS) The produced biplot in PCA represents well the distance among objects, but fails to represent the whole variation dimensions of the ordination space. Unlike PCA, non-metric multidimensional scaling (nMDS) does not to preserve the exact dissimilarities (distance) among objects in an ordination plot, instead it represents as well as possible the ordering relationships among objects in a small and specified number of axes. In other words, the goal of nMDS is to represent the original position of samples in multidimensional space as accurately as possible using a reduced number of dimensions. We can use the metaMDS() function in the vegan package to conduct non-metric multidimensional scaling in R. In addition to the input dataframe, this function also requires us to specify the distance measure distance = ? and number of reduced dimensions k = ?. [1] 0.1360859 From the nMDS results, we can extract the stress parameter. Stress identifies how well points fit within the specified number of dimensions. A good rule of thumb for stress: \\(&gt;0.2\\) Poor (risk in interpretation) \\(0.1-0.2\\) Fair (some distances misleading) \\(0.05-0.1\\) Good (inferences confident) \\(&lt;0.05\\) Excellent representation 7.4.1 Shepard Plot We can use a Shepard plot to learn about the distortion of representation. On the x-axis, it plots the original dissimilarity (original distances in full dimensions). On the y-axis, it plots the distances in the reduced dimensional space. Ideally, a really accurate dimension reduction will produce a straight line. 7.4.2 Plot ordination for nMDS No we can plot the ordination for nMDS, just like for PCA in the previous sections. The steps exactly the same. We use ordiplot() to create an empty canvas first, then use points() to add samples/variables. Additionally, we would like to represent species scores with arrows. 7.5 Other resources Multivariate analyses tutorials Building Skills in Quantitative Biology QCBS R Workshop Series - Multivariate Analyses in R Running NMDS using ‘metaMDS’(nMDS tutorial with vegan package) Introduction to Ordination Useful R packages factoextra (for visualizing PCA results) learnPCA (an R package for PCA learning) ggbiplot "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
