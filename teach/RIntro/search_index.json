[["index.html", "R Intro Workshop 1 Welcome to our Workshop! 1.1 Learning objectives", " R Intro Workshop The Cuddington Lab 06 July, 2024 1 Welcome to our Workshop! Before the workshop, please check out Get ready for the workshop. We would like you to download and install some files before the Workshop During the workshop, we will complete coding exercises that correspond to the Learning objectives below. We will focus on importing data, subsetting the data, completing various regression analysis and creating simple graphs. During the workshop this document will be available for you to copy code etc. After the workshop we will provide a more detailed version of this accompanying website. 1.1 Learning objectives Import, check and summarize data in R Create graphs to visualize data and models Apply linear, multiple and logistic regression models using R Interpret model fits and results Use simple principal component analyses "],["get-ready-for-the-workshop.html", "2 Get ready for the workshop 2.1 R from zero warmup 2.2 Install R 2.3 Download needed files and packages", " 2 Get ready for the workshop This is an introduction to basic stats in R with a focus on regression. We will assume a familiarity with R to the extent of the participant having tried to do something with the programming environment in the past (e.g., a t-test), but no more than this. 2.1 R from zero warmup If you have never used R in any way at a all, it may be helpful to take a quick look at this interactive tutorial for undergraduates at the University of Waterloo. Just complete the tutorial entirely online rather than downloading, starting from “R as a calculator”. https://shiny.math.uwaterloo.ca/biology/QBshiny/IntroR/ 2.2 Install R Then, when you are ready, get some help installing R on your laptop. You will need R installed to participate in workshop. Some resources to help are here: https://rstudio-education.github.io/hopr/starting.html 2.3 Download needed files and packages We will be using a flat .xlsx file of Hamilton Harbour data for our workshop exercise today. Our first job at the workshop will be to import that data. Please download the .xlxs to your personal computer prior to the workshop https://ecotheory.ca/teach/RIntro/ind dates flat all parameters Hamilton only FOR Kim.xlsx 2.3.1 Package to read .xlsx file In addition, we encourage you to download and install the packages we will be using for the workshop. R is an open source platform, which means that anyone can contribute code for everyone to use. There are thousands of functions in R written by scientists, statisticians and mathematicians all over the world. Sometimes a group of functions related to a particular task are grouped together in what is called a “library” or “package”. We are going to use one such library that has been created to import .xlsx files, “readxl”. On your own computer, you will need download the package and install it. This is pretty easy. On the RStudio menu select Tools, and then Install packages from the drop down list. Enter readxl in the search window, and then select Install. You can also use the function install.packages. Just type install.packages(“readxl”). The function install.package() downloads the indicated package from the internet and stores it. Eventually, in the console window you should see an indication the R has downloaded and installed the package. Once you’ve done this once, you don’t need to do it ever again, unless you update the version of R you are using, or wish to use an updated version of the package. 2.3.2 Other needed packages In addition, other exercises will use the vegan, and car libraries. Please install these packages prior to the workshop as well. 2.3.3 Load a package into memory These packages are now part of R’s set of functions, but since it they not part of the standard environment, you will have to mention to R when you wish to use functions from this collection. The library() function tells R you wish to use commands from a particular package on this occasion. Every time you restart R you will have to mention to R when you wish to use functions from this collection. The simplest method to do this is to always include the statement library(readxl) at the beginning of code that uses functions from this package. Here R loads the package into active memory. library(readxl) If this command works, you are ready to go for the first part of the workshop! "],["basic-r.html", "3 Basic R 3.1 Data import 3.2 Data in R 3.3 Functions in R 3.4 Plotting data 3.5 The Editor(Source) window 3.6 T-test and boxplot", " 3 Basic R 3.1 Data import Our first job today is to use a package to import the data we will be using throughout the workshop. If you have sucessfully installed the library readxl, we can use it to read in the .xlsx file. 3.1.1 Where’s my file?? To do the import we need the path where the file is located. There are at least three ways to find and import a file in R: use file.choose() to get an interactive menu, use the actual path to the file (e.g., “/Users/Kim/Documents/mydata.xlsx”), use setwd() to change the current directory to the location of the file Try it now: Import the data file using the function read_excel() from the readxl library, and handle the path location of the file using one of the methods above. # load the library library(readxl) # import the xlsx file ham &lt;- read_excel(&quot;ind dates flat all parameters Hamilton only FOR Kim.xlsx&quot;) 3.1.2 Did the import work?? Once you have read in the data, it is critically import to check that the import worked properly. Difficulties that can arise from incorrectly formatted files can then be resolved. Problems can include: numeric data being read in as character data, column names being read in as the the first row of data, missing values being coded as something other than NA. There are a number of commands to look at the dataset or a portion of it. Four quick methods to check data import: print the imported dataframe or use view(), colnames(), head() or tail(), str() Try it now: Use colnames() to see the names of the columns in ham, tail() to see the last rows of ham, and str() to see information about data type etc. # View(ham) # Look at the first 5 column names of the dataframe ham colnames(ham)[1:5] [1] &quot;waterbody&quot; &quot;area_group&quot; &quot;Latitude&quot; &quot;Longtitude&quot; &quot;Station_Acronym&quot; # Looks at the last 5 rows tail(ham, n = 5) # A tibble: 5 × 109 waterbody area_group Latitude Longtitude Station_Acronym report_Stn SamplingDate season year Julian_Day Julian_Week Month Station_depth `water level` Ammonia_ECCC1m DIC_ECCC1m DOC_ECCC1m POC_ECCC1m Chl_ECCC1m `Chl Cor_ECCC1m` NO2_NO3_ECCC1m PON_ECCC1m `TKN dissolved…` SRP_ECCC1m TP_ECCC1m `TP dissolved_…` Chl_a_uncorrect… Secchi `Kd light_atte…` Cyano Chloro Eugleno Chryso Diatom Crypto Dino Total_phyto Ceratium edible less_edible mixotroph toxin_producer colonial_BG filamentous_BG &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Hamilton Ha… west 43.3 -79.9 HHCarolsP CP 2020-10-26 00:00:00 4 2020 300 44 10 10 74.7 NA NA NA NA NA NA NA NA NA NA NA NA NA 2 1.08 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2 Hamilton Ha… west 43.3 -79.9 HHRHYC RHYC 2020-10-26 00:00:00 4 2020 300 44 10 9.7 74.7 NA NA NA NA NA NA NA NA NA NA NA NA NA 1.7 0.855 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 3 Hamilton Ha… west 43.3 -79.9 HHRHYC out RHYC out 2020-10-26 00:00:00 4 2020 300 44 10 12.6 74.7 NA NA NA NA NA NA NA NA NA NA NA NA NA 1.6 1.15 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 4 Hamilton Ha… wind 43.3 -79.8 HH4_PHYTO WSTP 2020-10-26 00:00:00 4 2020 300 44 10 8.8 74.7 NA NA NA NA 4.37 NA NA NA NA NA NA NA 4.37 1.2 2.34 328. 77.0 0 13.6 214. 52.4 18.9 703. 18.9 99.4 514. 52.2 254. 73.1 251. 5 Hamilton Ha… west 43.3 -79.9 HHBayfront-West BFW 2020-10-26 00:00:00 4 2020 300 44 10 6.3 74.7 NA NA NA NA NA NA NA NA NA NA NA NA NA 1.75 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA # … with 65 more variables: flagellate &lt;dbl&gt;, small_flagellate &lt;dbl&gt;, filamentous_Diatom &lt;dbl&gt;, `2-20um_BM` &lt;dbl&gt;, gt20um_BM &lt;dbl&gt;, lt2um_BM &lt;dbl&gt;, Bacteria_BM &lt;dbl&gt;, APP_BM &lt;dbl&gt;, HNF_BM &lt;dbl&gt;, ciliate_BM &lt;dbl&gt;, Bacterial_Growth_rate &lt;dbl&gt;, gt20um_rate &lt;dbl&gt;, `2-20um_rate` &lt;dbl&gt;, lt2um_rate &lt;dbl&gt;, sum_SFP_rate &lt;dbl&gt;, Rotifer &lt;dbl&gt;, Bosmina &lt;dbl&gt;, Chydorus &lt;dbl&gt;, Ceriodaphnia &lt;dbl&gt;, D.ambigua &lt;dbl&gt;, D.galeatamendotae &lt;dbl&gt;, D.retrocurva &lt;dbl&gt;, D.unknown &lt;dbl&gt;, Diaphanosoma &lt;dbl&gt;, # Eubosmina &lt;dbl&gt;, litoral.clad &lt;dbl&gt;, Bythotrephes &lt;dbl&gt;, Cercopagis &lt;dbl&gt;, Leptodora &lt;dbl&gt;, cycl.nauplii &lt;dbl&gt;, cycl.copepodite &lt;dbl&gt;, Diacyclops &lt;dbl&gt;, Acanthocyclops &lt;dbl&gt;, Mesocyclops &lt;dbl&gt;, cycl.unknown &lt;dbl&gt;, calan.nauplii &lt;dbl&gt;, calan.copepodite &lt;dbl&gt;, `Skistodiaptomus oregonensis` &lt;dbl&gt;, `Leptodiaptomus sicilus` &lt;dbl&gt;, calan.unknown &lt;dbl&gt;, veliger &lt;dbl&gt;, Daphnia &lt;dbl&gt;, bosminid &lt;dbl&gt;, other.herb.cladocera &lt;dbl&gt;, `predatory cladocera` &lt;dbl&gt;, `cladocera total` &lt;dbl&gt;, cyclopoid &lt;dbl&gt;, # calanoid &lt;dbl&gt;, total.zoop &lt;dbl&gt;, bottom_epi_depth &lt;dbl&gt;, bottom_meta_depth &lt;dbl&gt;, bottom_exo_depth &lt;dbl&gt;, watercolumn_temp &lt;dbl&gt;, mean_mixing_depth_temp &lt;dbl&gt;, meta_temp &lt;dbl&gt;, hypo_temp &lt;dbl&gt;, epi_DO &lt;dbl&gt;, meta_DO &lt;dbl&gt;, hypo_DO &lt;dbl&gt;, min_DO &lt;dbl&gt;, epi_chl &lt;dbl&gt;, meta_chl &lt;dbl&gt;, hypo_chl &lt;dbl&gt;, `bottom hypoxia (Y/N)` &lt;chr&gt;, stratified &lt;chr&gt; # get a summary regarding the first 5 columns str(ham, list.len = 5) tibble [742 × 109] (S3: tbl_df/tbl/data.frame) $ waterbody : chr [1:742] &quot;Hamilton Harbour&quot; &quot;Hamilton Harbour&quot; &quot;Hamilton Harbour&quot; &quot;Hamilton Harbour&quot; ... $ area_group : chr [1:742] &quot;deep&quot; &quot;NE&quot; &quot;deep&quot; &quot;west&quot; ... $ Latitude : num [1:742] 43.3 43.3 43.3 43.3 43.3 ... $ Longtitude : num [1:742] -79.9 -79.8 -79.8 -79.9 -79.9 ... $ Station_Acronym : chr [1:742] &quot;HH908&quot; &quot;HH6&quot; &quot;HH258&quot; &quot;HHBayfront&quot; ... [list output truncated] 3.2 Data in R 3.2.1 Data types The str() function above gave us information regarding the type of data in each column of our imported dataframe. There are 4 main data types we will be using today: numeric, character, factor and logical. numeric data are numbers (e.g., 3, 10.5, 1E8) character data sometimes called “strings” are words or alphanumeric codes, indicated with double quotation marks (e.g. “k”, “R is exciting”, “FALSE”, “11.5”) factor data are a special case of character variables, and are used when there are a limited number of unique character strings. This data type is used to represent categorical data (e.g., “male” and “female”) logical data are sometimes called “boolean”. This is data with only two values: TRUE or FALSE. Sometimes after data import we will need to convert the data from one type to another. For example, if we wanted to do an analysis that compared data from different stations we would want to convert the column Station_Acronym from data type character to data type factor. This is easy to do using the function as.factor(), and accessing just that column (see below) 3.2.2 Data structures You’ll notice that the data are structured in columns. This is a dataframe, one of the most used data structures in R. Data structures are sets of variables organized in a particular way. In R there are 4 primary data structures we will use repeatedly: vectors, matrices, dataframes and lists. Vectors are one-dimensional ordered sets composed of a single data type. Data types include integers, real numbers, and strings (character variables). For example, a single column of the imported data is a vector. Matrices are two-dimensional ordered sets composed of a single data type (e.g., all numeric) equivalent to the concept of matrix in linear algebra. For example, a set of correlation coefficients for different parameters is a matrix. Dataframes are one to multi-dimensional sets with a row-column structure, and can be composed of different data types (although all data in a single column must be of the same type). In addition, each column in a data frame may be given a label or name to identify it. Dataframes are equivalent to a flat file database, similar to spreadsheets (e.g., like a single excel spreadsheet). For example, our inputted dataset is a dataframe. Lists are compound objects of associated data. Like dataframes, they need not contain only a single data type, but can include strings (character variables), numeric variables, and even such things as matrices and data frames. In contrast to dataframes, list items do not have a row-column structure, and items need not be the same length; some can be a single value, and others a matrix. You can think of a list as a named box to put related objects into. For example, the R output of a linear regression is a list. 3.2.3 Selecting portions of a dataframe To start our examination and analysis of this data, we need to be able to select items of interest. Dataframes are indexed by rows and columns. To grab the item from the 5th row and 2nd column: type mydata[5,2] one column you can type either: mydata[,2], which grabs everything in column 2, or if you know the name of the column you can use that as: mydata$Population or mydata[,“Population”] to get rows 2 to 5 you can enter: mydata[2:5,2]. You can also combine this with the column name which may be easier to read, as mydata$Population[2:5]. Try it now: Select just the first 5 rows and first 3 columns of the dataframe you read into R. # access a subsection of a dataframe (first 5 rows, and first # 3 columns) ham[1:5, 1:3] # A tibble: 5 × 3 waterbody area_group Latitude &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 Hamilton Harbour deep 43.3 2 Hamilton Harbour NE 43.3 3 Hamilton Harbour deep 43.3 4 Hamilton Harbour west 43.3 5 Hamilton Harbour deep 43.3 Try it now: Access the Station_Acronym column of the data. Remember, we can also just grab a single column, using the column name to identify it as mydata$thiscolumn or mydata[,“thiscolumn”]. Next use the as.factor() function to convert this data to factor instead of character. We’ll also use the levels() function to see how that worked # select by column and print the first 6 rows head(ham$Station_Acronym) [1] &quot;HH908&quot; &quot;HH6&quot; &quot;HH258&quot; &quot;HHBayfront&quot; &quot;HH908&quot; &quot;HH9031&quot; # create a new variable that saves same data converted to # factor fac_Station_Acronym = as.factor(ham$Station_Acronym) # see the levels of the new factor variable levels(fac_Station_Acronym) [1] &quot;CCIW dock&quot; &quot;HH17&quot; &quot;HH1B&quot; &quot;HH2&quot; &quot;HH2000&quot; &quot;HH2001&quot; &quot;HH2002&quot; &quot;HH2003&quot; &quot;HH2004&quot; &quot;HH258&quot; &quot;HH2B&quot; &quot;HH39&quot; &quot;HH4_PHYTO&quot; &quot;HH6&quot; &quot;HH8&quot; &quot;HH9031&quot; &quot;HH9033&quot; &quot;HH908&quot; &quot;HH917&quot; &quot;HHBayfront&quot; &quot;HHBayfront-West&quot; &quot;HHBFouter&quot; &quot;HHBURSTP&quot; &quot;HHCarolsP&quot; &quot;HHRHYC&quot; &quot;HHRHYC out&quot; &quot;HHWC&quot; We can also create new data subsets by using the c() or combine function. Try it now: Let’s access the first 5 rows, the first 3 columns and columns 9-10, and save the result to a new variable ’ ham_sub’ # save a subsection of a dataframe ham_sub = ham[1:5, c(1:3, 9:10)] ham_sub # A tibble: 5 × 5 waterbody area_group Latitude year Julian_Day &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Hamilton Harbour deep 43.3 2016 131 2 Hamilton Harbour NE 43.3 2021 174 3 Hamilton Harbour deep 43.3 2021 174 4 Hamilton Harbour west 43.3 2022 174 5 Hamilton Harbour deep 43.3 2022 174 Try it now: Our subsetted data is not ordered by year or group, so let’s use the order() function to rearrange # save a subsection of a dataframe ham_sub = ham_sub[order(ham_sub$area_group, ham_sub$year), ] ham_sub # A tibble: 5 × 5 waterbody area_group Latitude year Julian_Day &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Hamilton Harbour deep 43.3 2016 131 2 Hamilton Harbour deep 43.3 2021 174 3 Hamilton Harbour deep 43.3 2022 174 4 Hamilton Harbour NE 43.3 2021 174 5 Hamilton Harbour west 43.3 2022 174 3.2.4 Using conditional statements to access data We can also use conditional statements to access portions of the data. Conditional statements evaluate to TRUE or FALSE. The “==” symbol is used to determine if a variable is equal to some value, while “!=” evaluates is something is not equal. As you might expect we can also use greater than (“&gt;”) or less than (“&lt;”) conditionals. Try it now: First check out how the conditional statement produces a logic vector of TRUE and FALSE by entering ham$Station_Acronym==“HH6”. Then use this logical vector to subset the calanoid vector to choose only rows where the Station_Acronym is “HH6” (i.e., only the TRUE items will be included) Enter ham$calanoid[ham$Station_Acronym==“HH6”] and examine the data. # create a logical vector using a conditional, and look at # the first 6 items head(ham$Station_Acronym == &quot;HH6&quot;) [1] FALSE TRUE FALSE FALSE FALSE FALSE # by condition... using head() to show the first 6 items head(ham$calanoid[ham$Station_Acronym == &quot;HH6&quot;]) [1] 8.41e+01 1.32e+01 6.41e+01 5.84e+01 2.53e+01 6.37e-03 Of course, this gives us all of the sampling dates for this station. Let’s assume we only want data from 2016. In that case we can combine our conditional statements using “&amp;” for AND and “|” for OR. Try it now: Use an “&amp;” symbol to indicate that we want the selected data to be from station “HH6” and to be from the 2016. ham$calanoid[ham$Station_Acronym == &quot;HH6&quot; &amp; ham$year == 2016] [1] 64.15 25.26 70.72 142.45 89.27 70.07 32.33 78.56 7.97 120.23 91.17 19.29 104.39 If we wanted we could use the same conditionals to grab the julian sampling day for these observations, and save the result to new vectors cal_HH6_2016 = ham$calanoid[ham$Station_Acronym == &quot;HH6&quot; &amp; ham$year == 2016] jul_cal_HH6_2016 = ham$Julian_Day[ham$Station_Acronym == &quot;HH6&quot; &amp; ham$year == 2016] Table 3.1: Calanoid data for 2016 at station HH6 calanoid julian 64.14 145 25.26 131 70.72 160 142.45 188 89.27 201 70.07 215 32.33 174 78.56 258 7.97 228 120.23 242 91.17 270 19.29 286 104.39 299 Try it now: Access the calanoid data at station “HH6” for 2014 [1] 3.753 4.420 4.472 49.364 7.176 0.838 3.705 6.729 4.091 5.796 1.015 5.420 3.3 Functions in R Next, let’s figure out how to complete some simple calculations with these values, like finding the mean and standard deviation. We have already used some functions in R. R contains thousands of functions, and more are being added everyday. If you are trying to find a function that does something you need to do, you can use the command apropos(“keyword”) to see if there is a function that contains the keyword in its name. Try it now: For example if you wanted to find a function that calculated means you might: type ‘apropos(“mean”)’. What happens? apropos(&quot;mean&quot;) [1] &quot;.colMeans&quot; &quot;.rowMeans&quot; &quot;colMeans&quot; &quot;kmeans&quot; &quot;mean&quot; &quot;mean_cl_boot&quot; &quot;mean_cl_normal&quot; &quot;mean_mixing_depth_temp_values&quot; &quot;mean_sdl&quot; &quot;mean_se&quot; &quot;mean.Date&quot; &quot;mean.default&quot; &quot;mean.difftime&quot; &quot;mean.POSIXct&quot; &quot;mean.POSIXlt&quot; [16] &quot;meandist&quot; &quot;rowMeans&quot; &quot;weighted.mean&quot; One of the listed functions “mean” looks promising. Your next step might be to get more information by typing help(mean) or ?mean or example(mean). Try it now: You should see that the function mean() works on a vector of numeric data. Try it now: Use these tools to find a function that calculates standard deviation. Oh no! What happens? Perhaps the function does not contain the keyword in its name, in this case try ??”keyword phrase” to get R to search function descriptions If the internal help menu lets you down you can also try a search at http://www.rseek.org/ Try it now: Once you have found the function name, use it and the mean() to get the mean and standard deviation of calanoid density in 2014. [1] 8.06 [1] 13.1 3.3.1 Summarizing data Other functions can help with quick data summaries. for example, the table() function can be used to take a quick look at the number of sampling dates for each station. Try it now: Use this function on the Station_Acronym column of the data Station Name No. of samples CCIW dock 3 HH17 13 HH1B 22 HH2 1 HH2000 1 HH2001 1 HH2002 1 HH2003 1 HH2004 1 HH258 172 HH2B 21 HH39 1 HH4_PHYTO 22 HH6 136 HH8 38 HH9031 22 HH9033 22 HH908 96 HH917 22 HHBayfront 22 HHBayfront-West 22 HHBFouter 3 HHBURSTP 22 HHCarolsP 22 HHRHYC 21 HHRHYC out 21 HHWC 13 The summary() function can also give information about a vector. Try it now: Use the summary function on the “Chl_a_uncorrected” data for site HH258 Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 3.1 9.5 12.6 15.0 19.3 70.0 25 3.4 Plotting data A quick check of data can also be done with simple plots. for example, let’s see what the Chl_a_uncorrected data by julian day looks like using the plot() function. plot(y = ham$Chl_a_uncorrected, x = ham$Julian_Day) Try it now: Create this plot Of course, this is the data from every station. Let’s just plot station HH258, and colour code by sampling year sub2_ham = ham[ham$Station_Acronym == &quot;HH258&quot;, ] plot(y = sub2_ham$Chl_a_uncorrected, x = sub2_ham$Julian_Day, col = sub2_ham$year) Try it now: Create this plot Kind of hard to read… I think I want to group the data by decade, and let’s pimp the plot a little. We also need axis labels, a nicer symbol and a legend. Wow! That’s a lot of things to do. Time to start saving our code using the Editor window. 3.5 The Editor(Source) window Previously you could have completed all of these commands in the Console window where R would execute them immediately. Now we’re going to have a bunch of commands, some of which we may want to tweek (e.g., colours). This time, let’s save our code in the Editor or Source window so that it is easy to change and rerun. We’ll create a new file using the drop down menus. Click File then New File, and then R Script. A blank document will appear in the Editor window is found in the top left of the default R Studio. Try it now: Create a new file, enter the following code, and then save it using the drop down menu or cntrl-s or command-s. Then run the code by selecting it and hitting Run in the menu or command-enter. sub2_ham$decade &lt;- cut(sub2_ham$year, breaks = c(2e+03, 2010, 2020, 2030), labels = c(2e+03, 2010, 2020)) plot(y = sub2_ham$Chl_a_uncorrected, x = sub2_ham$Julian_Day, col = sub2_ham$decade, pch = 16, xlab = &quot;Chla&quot;, ylab = &quot;Julian Day&quot;, bty = &quot;l&quot;, cex.lab = 1.6, las = 1) legend(&quot;topright&quot;, legend = levels(sub2_ham$decade), pch = 16, col = 1:3, bty = &quot;n&quot;) Notice that I’ve created and used a factor variable by using the cut() function. I’ll use this to control the colour of the data from each decade. I’m also using some of the formatting possibilities in the plot() function. There’s LOTS of options: take a look at ?plot.default and ?par to get some ideas. For example, symbol shape is controlled by the pch option in the plot command. Let’s try one more. Let’s quickly visualize the number of calanoid samples from each area_group by date. Again we will use a factor variable to control appearance. ham$area_group = as.factor(ham$area_group) plot(calanoid ~ SamplingDate, data = ham, col = area_group) legend(&quot;topleft&quot;, legend = levels(ham$area_group), bty = &quot;n&quot;, pch = 1, col = c(1:length(levels(ham$area_group)))) Try it now: Create this plot This is too cluttered, so I am going to summarize the data by year and area, and then plot that. I’m going to plot the data from each area individually starting with the first one, and then adding subsequent areas to the same plot using the lines() function. samps = as.data.frame(table(ham[, c(&quot;year&quot;, &quot;area_group&quot;)])) samps$year = as.numeric(as.character(samps$year)) plot(Freq ~ year, data = samps[samps$area_group == &quot;deep&quot;, ], bty = &quot;L&quot;, xlim = c(min(samps$year), max(samps$year)), ylim = c(min(samps$Freq), max(samps$Freq)), col = &quot;black&quot;, type = &quot;b&quot;, main = &quot;Calanoid samples per year by area group&quot;) lines(Freq ~ year, data = samps[samps$area_group == &quot;NE&quot;, ], col = &quot;red&quot;, type = &quot;b&quot;) lines(Freq ~ year, data = samps[samps$area_group == &quot;west&quot;, ], col = &quot;blue&quot;, type = &quot;b&quot;) lines(Freq ~ year, data = samps[samps$area_group == &quot;wind&quot;, ], col = &quot;green&quot;, type = &quot;b&quot;) legend(&quot;topleft&quot;, legend = levels(ham$area_group), lty = 1, bty = &quot;n&quot;, pch = 1, col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)) Try it now: Create this plot At this point, the plotting is getting a bit more complex, so you may want to use another library called ggplot2 to do some of your plotting. We won’t use this today, since this library of functions has a non-intuitive command structure, but is quick for plotting multiple sets of data on the same plot with a legend. # similar plot using ggplot library(ggplot2) ggplot(samps, aes(x = year, y = Freq, group = area_group, colour = area_group)) + geom_line() 3.6 T-test and boxplot Finally, let’s create a box plot. year_sub = ham[(ham$year == 2019), ] boxplot(calanoid ~ area_group, data = year_sub) Also note, that we can easily complete statistical tests other than regression in R. For example, here’s a t-test comparing calanoid data at “deep” and “NE” sites. t.test(calanoid ~ area_group, data = year_sub[(year_sub$area_group == &quot;deep&quot; | year_sub$area_group == &quot;NE&quot;), ]) Welch Two Sample t-test data: calanoid by area_group t = -2, df = 15, p-value = 0.2 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -19.20 3.24 sample estimates: mean in group deep mean in group NE 8.47 16.45 Fanastic! You now have the basics of importing, manipulating, and plotting in R. "],["linear-regression.html", "4 Linear regression 4.1 Plot the data and predictions 4.2 Plot the data and predictions", " 4 Linear regression 4.0.1 Introduction A linear regression estimates the parameters of a linear relationship between a response (y, the depdendent variable) and an explanatory factor (x, an independent variable) as: \\[y=\\beta_0+\\beta_1x+\\epsilon\\] Significance testing is used to determine if the slope of this relationship is significantly different than zero. For example, for our Hamilton Harbour data, we will be testing for a relationship between water level (our predictor) and total zooplankton counted in a net grab (our response). 4.0.2 Linear regression using water level and total zooplankton The lm() (or linear model) function runs a linear regression where we indicate the response and predictor variable using the “~” symbol. Try it now Use the code below to regress total zooplankton on water level. Notice we will change the column names with the spaces first using the sub() function (i.e., replace “water level” with “water.level”), since spaces are always a pain when using code. # change column names to replace space with &#39;.&#39; colnames(ham) = sub(&quot; &quot;, &quot;.&quot;, colnames(ham)) # save linear regression results waterlzoo.lm &lt;- lm(total.zoop ~ water.level, data = ham) # print regression results waterlzoo.lm Call: lm(formula = total.zoop ~ water.level, data = ham) Coefficients: (Intercept) water.level -30814 415 Notice that when we simply try to print the regression object, the output is very brief: just the intercept and slope estimate. The regression object is a list data structure and actually does contain a lot of information. We can access a nicely formatted summary of all this information using the summary() function. Try it now Use the summary() function on the list data structure of your regression output # print a summary of the regression results sum_lm = summary(waterlzoo.lm) sum_lm Call: lm(formula = total.zoop ~ water.level, data = ham) Residuals: Min 1Q Median 3Q Max -618 -184 -85 76 3515 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.08e+04 3.92e+03 -7.87 2.0e-14 *** water.level 4.16e+02 5.22e+01 7.95 1.1e-14 *** --- Signif. codes: 0 &#39;***&#39; 1e-03 &#39;**&#39; 1e-02 &#39;*&#39; 5e-02 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 382 on 537 degrees of freedom (203 observations deleted due to missingness) Multiple R-squared: 0.105, Adjusted R-squared: 0.104 F-statistic: 63.3 on 1 and 537 DF, p-value: 1.08e-14 Let’s go through this output. We can: check for the significance of the relationship, determine whether the relationship between the predictor and response is positive or negative, and determine how much of the variance in the data is explained by this model. This first thing to notice is whether or not we have a signficant relationship. The output provides a t-value and probability for the null hypothesis that the intercept and slope have values of zero. We can see the p-value for the slope test is 1.078^{-14} and indicates a significant relationship. Notice that if we just wanted to grab some of this information (e.g., to create a table in a publication) we can just subset the list object Try it now Subset the list object to get the information about the slope, and the R2 value. sum_lm$coefficients[2, ] Estimate Std. Error t value Pr(&gt;|t|) 4.15e+02 5.22e+01 7.95e+00 1.08e-14 sum_lm$adj.r.squared [1] 0.104 The estimate of the slope is positive, although the model does not explain much of the variance in the data. This could be because we have violated one or more of the assumptions of the regression. And of course whenever we use a regression we should test to see if we have met these asumptions. 4.0.3 What are the assumptions of a linear regression? The most basic assumptions we make when we use linear regression is that there is indeed a linear relationship (rather than a non-linear relationship) between the predictor and the response. However, there are also assumptions about the variance in the data. We also assume: Variance in the data about this linear relationship is normally distributed Variance is consistent across all the values of the independent variable (e.g., there is not greater variance for larger values of the predictor). Each observation is independent of one another (there is no significant correlation between the independent variable values) 4.0.4 How do you evaluate the assumptions of a linear regression? We can get a quick assessment of whether we have violated some of these assumptions by creating diagnostic plots. These plots display the residuals of the linear regression in various ways. To see these plots we can just use the following plot() command, and hit return to display each of the four plots in turn. plot(waterlzoo.lm) a. The first plot, Residuals vs Fitted, shows the regression residuals vs the fitted values.The residuals should be randomly distributed around the horizontal line representing a residual error of zero. There should not be a distinct trend in the distribution of points that might indicate non-linearity. The red line provides a summary of the distribution of points. On our plot this line slopes downwards. b. The Q-Q plot evaluates our second assumption: that the data variance about the fitted line is normally distributed. If the points on the plot follow the diagonal line, then the residuals are normally distributed. We can see here that we are not doing well, there is a big departure from the expected distribution. c. The Scale-Location plot is used to determine if our third assumption that the variance of the residuals is be consistent across all the values is being met (i.e., the variance in the data is homoscedastic). Again, there should be no obvious trend in this plot, but we are seeing an upward tilt indicating more variance at larger values of our predictor variable. d. The Residuals versus Leverage plot in the lower right shows each points leverage, which is a measure of its importance in determining the regression result. This plot can be used to determine if there are any highly influential points in the dataset (i.e. outliers), and is also an evaluation of our third assumption. Superimposed are the contour lines for the Cook’s distance, which is another measure of the importance of each observation to the regression. Smaller distances means that removing the observation has little effect on the regression results. Distances larger than 1 are suspicious and suggest the presence of a possible outlier or a poor model. We have one or two influenctial points. Here’s another example from a regression of Chl a vs TP at station HH6, which shows very clearly that one point in particular is having a big influence on the regression. Actually, we probably should have plotted the data first to get some idea of what the data were like. Let’s do that now. Try it now: Plot our data plot(x = ham$water.level, y = (ham$total.zoop), xlab = &quot;Water level (m)&quot;, ylab = &quot;Total zooplankton&quot;) The plot explains a lot of our problems with the model assumptions. There are some very large values relative to most observations. In general, count and biomass data has this kind of distribution: a lot of small values and a few very large ones so that the data variance about the fitted line is non-normal. We can transform the population data using a natural logarithm to try and improve the normality. This is easy to do with the log() function. So we will run the regression on this transformed data and see if we are doing a better job meeting the model assumptions. Try it now Run a regression on log-transformed data # save linear regression results log_waterlzoo.lm &lt;- lm(log(total.zoop) ~ water.level, data = ham) summary(log_waterlzoo.lm) Call: lm(formula = log(total.zoop) ~ water.level, data = ham) Residuals: Min 1Q Median 3Q Max -3.296 -0.554 0.073 0.663 2.744 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -73.921 10.063 -7.35 7.6e-13 *** water.level 1.057 0.134 7.87 1.9e-14 *** --- Signif. codes: 0 &#39;***&#39; 1e-03 &#39;**&#39; 1e-02 &#39;*&#39; 5e-02 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.982 on 537 degrees of freedom (203 observations deleted due to missingness) Multiple R-squared: 0.103, Adjusted R-squared: 0.102 F-statistic: 62 on 1 and 537 DF, p-value: 1.92e-14 We see that our relationship is still signficant and positive under the transform. Let’s check the diagnostic plots quickly # save linear regression results plot(log_waterlzoo.lm) Our diagnostic plots look much better, particularly with respect to the normality of the resiudals. 4.1 Plot the data and predictions Let’s create a plot with our fitted model shown together with the data. the data, and superimpose our fitted model 4.1.1 Get model predictions and confidence intervals We will use the predict() function to calculate confidence intervals about our regression, and superimpose the model on the data plot using the lines() function. First let’s just get the predicted regression line and the confidence intervals about this line. We do this by creating a continuous sequence of x values, and applying our model to these values. Try it now # creating a vector &#39;x&#39; to predict total.zoop from waterlevel # get min water level min_water = min(ham$water.level, na.rm = TRUE) # get max water level max_water = max(ham$water.level, na.rm = TRUE) # build a new data frame that has vector of xvalues from min # to max in increments of 0.05 # create the sequence x &lt;- seq(min_water, max_water, by = 5e-02) # create a new dataframe with this predictor ndata = data.frame(water.level &lt;- x) # use this dataframe and the predict function to calculate # confidence intervals confidenceint &lt;- predict(log_waterlzoo.lm, newdata = ndata, interval = &quot;confidence&quot;, level = 0.95) cf = as.data.frame(confidenceint) 4.2 Plot the data and predictions We can then create a plot with these model predictions. Try it now Plot! # create a plot with water level and total zooplankton use # xlab(), ylab() and main() to put titles on the plot plot(x = ham$water.level, y = log(ham$total.zoop), xlab = &quot;Water level (m)&quot;, ylab = &quot;(log) Total zooplankton&quot;, main = &quot;Relationship between water level and total zooplankton&quot;) # add regression line to the plot lines(cf$fit ~ x, col = &quot;blue&quot;, lty = 1) # lower bound confidence interval lines(cf$lwr ~ x, col = &quot;blue&quot;, lty = 3) # upper bound confidence interval lines(cf$upr ~ x, col = &quot;blue&quot;, lty = 3) 4.2.1 Common mistakes Be careful when coding a linear regression using the lm() function that the independent and dependent variables are coded in the right order. i.e. lm(dependent variable~independent variable, data = yourdata). 4.2.2 References and resources Phillips, N. D. (2018, January 22). Yarrr! The Pirate’s Guide to R. YaRrr! The Pirate’s Guide to R. https://bookdown.org/ndphillips/YaRrr/ * Check out chapter 15 on linear regression analysis "],["multiple-linear-regression-model.html", "5 Multiple Linear Regression Model 5.1 Introduction 5.2 Data preparation 5.3 Model building 5.4 Model output 5.5 What could go wrong? 5.6 References and Resources", " 5 Multiple Linear Regression Model 5.1 Introduction 5.1.1 What is multiple linear regression model Multiple linear regression model allows using multiple predictor variables to predict a single response variable, as in this equation: \\[\\hat{y}=\\hat{a}x_1+\\hat{b}x_2+\\hat{c}x_3...+\\hat{z}\\] It’s basically an extension of the previously introduced single-variate linear regression model, both mathematically speaking and in the sense of building them in R. 5.1.2 Recipe for multiple linear regression model in R To build and interpret a multiple linear regression model: First, we will start from preparing data for building model. We will select and check our variables of interests in Data preparation. The characteristics of our data might affect the subsequent model building and evaluation process. Once we prepared the data, we can start to build the actual model. The standard R package stats provides the simplest way to do single and multiple linear regression model through the function lm(). We will introduce the detailed usage of this function for multiple linear regression model in Model building. At the end, we examine our model and analyze the presented relationships between the predictors and the response. An entire section (Model output) will be dedicated in introducing ways to interpret the model output and to evaluate the model based on all the assumptions multiple linear regression model made. Try it now: Before starting the coding process: There are two necessary packages readxl and car for this section. If you haven’t loaded these packages, please load them now so they won’t cause errors in the following sections Note: I hide the warnings from this code chunk because loading packages give out a lot of warnings. If you want to see those warnings, delete warning=FALSE. For this and following sections, we will be using 3 predictor variables (water level, dissolved inorganic carbon amount and bottom hypoxia) and 1 response variable (total zooplankton) from the Hamilton Harbour AOC project dataset. So, we need to separate our selected variables from the complete dataset: # Select and combine variables of interests multi_data &lt;- ham[, c(14, 16, 108, 93)] # Label each column of our new dataset colnames(multi_data) &lt;- c(&quot;waterlevel&quot;, &quot;DIC&quot;, &quot;bottomhypoxia&quot;, &quot;totalzoop&quot;) # remove NAs multi_data &lt;- na.omit(multi_data) 5.2 Data preparation First, we want to determine the characteristics of the three predictor variables we have, most importantly, are they continuous or categorical? Continuous variable: data that are measured and ordered. It can be any specific value within a certain numerical range. For example: table(multi_data$waterlevel) 74.34 74.42 74.5 74.54 74.55 74.56 74.59 74.6 74.65 74.7 74.71 74.74 74.78 74.79 74.81 74.84 74.85 74.86 74.88 74.9 74.91 74.92 74.95 74.96 74.98 75.02 75.03 75.05 75.06 75.08 75.09 75.1 75.12 75.14 75.16 75.17 75.18 75.19 75.24 75.3 75.43 75.53 75.69 75.8 2 2 4 6 3 4 3 6 4 8 4 4 10 10 7 11 5 6 4 4 6 2 12 4 4 8 4 4 4 1 12 2 7 5 6 4 2 8 2 4 1 2 2 4 Categorical variable: data that is divided into categories with distinct label. These data can’t be ordered or measured as continuous variables. For example: table(multi_data$bottomhypoxia) n y 116 101 5.2.1 Check data type Because of the intrinsic differences between continuous and categorical variables, we always want to check whether our data is in the correct data type before proceeding to the next step. Sometimes the data is not read in as the correct data type, and other times data types got altered during previous data modification procedures. We can check data type easily using the class() function, for example: class(multi_data$waterlevel) [1] &quot;numeric&quot; Try it now: Let’s continue to check the other three columns of our data frame using the same code: # Use the class() function to check for the other columns: class() The numeric variables are in their correct data type. If we ever want to reset them, we can set the data type to numeric using as.numeric(). For our categorical variables, we can transform them into groups/factors using as.factor(). # Set bottom hypoxia column as &#39;factor&#39; data type using # as.factor(model_name$variable_name) multi_data$bottomhypoxia &lt;- as.factor(multi_data$bottomhypoxia) Note: As you noticed, the as.xxx() format is used to transform data type directly in R. You can explore some other options on your own, such as as.character(), as.integer()…. 5.3 Model building Generating the multiple linear regression model itself is similar to the single variable model and is very simple once we prepared our data. Simply adding a + sign between predictor variables to include more than one variable in your model. So, instead of lm(response ~ predictor, data = dataframe), we are now using lm(response ~ predictor 1 + predictor 2 ..., data = dataframe) Try it now: Let’s try to include the two additional predictor variables DIC and bottomhypoxia in our model based on the previous single variate model: # Modify the below single variate linear regression model and # make it a multiple linear regression model: Hint: start by # adding a plus sign and another predictor after water level multi_model &lt;- lm(totalzoop ~ waterlevel, data = multi_data) 5.4 Model output 5.4.1 Model assumptions Just like the simple linear regression model, the multiple linear regression models come with similar assumptions: The predictor variables and the response variable have a linear relationship since we are doing a linear regression model. The model also assumed normal distribution in residuals. Homoscedasticity that there exists equal variance in residuals. And the multiple predictors aspect in this model brings in one more assumption: No multicollinearity. The independent variables should be independent of each other. 5.4.2 Evaluate model assumptions I found it much easier to build the model first then check the assumptions as we can utilize these auto-generated model diagnosis plots from the lm() function. Just like in the simple linear regression model, these plots can be called by simply run plot(model_name). Let’s try to do this: Try it now: # Access the diagnosis plots that are auto-generated from the # lm() function Using plot(model_name): plot() You should see four diagnosis plots generated: Residuals vs Fitted plot, Normal Q-Q plot, Scale-Location plot, and The Residuals vs Leverage plot. As in the simple linear regression model, these plots can help us determine whether the first three of the four assumptions are met for our model. The last assumption, Multicollinearity, can be checked by the following several ways depending on the data characteristics: If we only have continuous variables in our dataset, we can check this using correlation matrix through cor() or/and corrplot(), or variance inflation factor using vif() from the car package, or vifcor() and vifstep() from the usdm package. If we have both continuous and categorical variables in our dataset, the vif() function will automatically detect our categorical variables and compute us the generalized vifs. Try it now: # Let&#39;s use vif(model_name) to calculate vif for our # variables vif() The result for our variables should look like this: waterlevel DIC bottomhypoxia 1.58 1.78 1.17 Our variables look good. If multicollinearity is found (vif &gt; 5 is a common threshold to use for indicating a problematic amount of multicollinearity), we can try to remove it by identify and remove the predictor variable that is causing troubles. 5.4.3 Output interpretation To access model output, the easiest way is to call the summary() method just like the simple linear regression model: summary(multi_model) Call: lm(formula = totalzoop ~ waterlevel + DIC + bottomhypoxia, data = multi_data) Residuals: Min 1Q Median 3Q Max -479.5 -169.1 -65.3 80.0 2158.8 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.05e+03 6.69e+03 -0.16 8.8e-01 waterlevel 9.48e+00 9.10e+01 0.10 9.2e-01 DIC 2.78e+01 7.88e+00 3.52 5.2e-04 *** bottomhypoxiay -4.89e+01 4.10e+01 -1.19 2.3e-01 --- Signif. codes: 0 &#39;***&#39; 1e-03 &#39;**&#39; 1e-02 &#39;*&#39; 5e-02 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 279 on 213 degrees of freedom Multiple R-squared: 0.119, Adjusted R-squared: 0.107 F-statistic: 9.59 on 3 and 213 DF, p-value: 5.72e-06 The interpretation of the model result is the same as in the simple linear regression model. For categorical variables, the significance level is calculated against a chosen baseline category. In this case, the n category in bottom hypoxia variable is the baseline category. Unlike the single variate model, you can compare the coefficients and significance of multiple predictors and decide on whether you want to exclude some of the variables in your final model. In this case, DIC seems to be the most significant variable and we might be able to get a similarly accurate model from this single predictor. 5.5 What could go wrong? Check carefully whether the values in your data set are in their correct data types, especially for the categorical variables if you have any. Be careful with the comma , and tilde ~ symbols when you are dealing with functions in R. They represent opposite positions of the independent and dependent variables. Always check the model assumptions to see if linear regression model is indeed the appropriate model to use. 5.6 References and Resources Other resources on introductions to multivariate linear regression model in R: https://www.datacamp.com/tutorial/multiple-linear-regression-r-tutorial https://bookdown.org/jimr1603/Intermediate_R_-_R_for_Survey_Analysis/regression-model.html#multiple-linear-regression https://library.virginia.edu/data/articles/getting-started-with-multivariate-multiple-regression https://rpubs.com/bensonsyd/385183 If you want to learn more about plotting prettier plots for presentation purposes, check the ggplot2 package: https://ggplot2.tidyverse.org/ If you want to learn more about the dummy coding process in lm() function that treat categorical variables, check the contrasts() function. "],["logistic-regression.html", "6 Logistic regression 6.1 What is a logistic regression? 6.2 How are logistic regressions different from linear regressions? 6.3 How does this type of data look like? 6.4 What does the logistic curve mean? 6.5 Aquatic ecology studies using logistic regressions 6.6 What kind of predictors can we have in a logistic regression? 6.7 Mathematical representation 6.8 Key binomial logistic model assumptions 6.9 Creating a logistic model in R 6.10 Additional resources", " 6 Logistic regression 6.1 What is a logistic regression? Association between binary response variable and predictors Example: species distribution models Binary response variable: presence (1) or absence (0) of species in area Predictions of habitat suitability 6.2 How are logistic regressions different from linear regressions? Linear regression - Quantitative predictions: straight line - Correlation and test for significance of regression - Model comparison (eg., single versus several predictors) Logistic regression - We can also do all of that! - Main difference: our outcomes are now binary - Examples of binary outcomes: presence versus absence; positive to a disease versus negative; dead versus alive - Outcomes can be categorized as 1 (e.g., success) and 0 (e.g., failure) 6.3 How does this type of data look like? Hamilton Harbour dataset: let’s suppose that it becomes hard for zooplankton to eat when there are more “less edible” algae than “edible” algae in the environment That was the criteria for populating the column “Easy to eat”: Nitrate/nitrite Edible Less edible Easy to eat 2.23 885 1646 no 2.50 900 1133 no 2.50 923 940 no 2.13 1546 1312 yes 2.13 812 454 yes 2.62 339 308 yes 2.67 1377 552 yes When we plot this type of binary data, we see that observations are either of the two outcome possibilities: This type of data is best fit by an s-shaped curve instead of a line. And this is another difference between linear and logistic regressions. 6.4 What does the logistic curve mean? It represents the probability of positive outcomes depending on predictors As we move along the curve and our predictor values change, we go from 0 to 100% probability of our outcome 6.5 Aquatic ecology studies using logistic regressions The distribution of gammarid species was predicted using logistic regressions, where current velocity was the most important factor explaining their distribution (Peeters &amp; Gardeniers, 1998) Foraging shift behaviour from the benthos to the water surface in brown trout (1 = surface prey consumed, 0 = no surface prey consumed) was predicted using fish length as a predictor in a logistic regression (Sánchez-Hernández &amp; Cobo, 2017) Amphipod Gammaridae; Brown Trout, USFWS Mountain-Prairie 6.6 What kind of predictors can we have in a logistic regression? Just like in a linear regression, we can use continuous and/or discrete variables to make predictions. Here are some examples: Continuous variables Temperature Precipitation Water depth Nitrogen concentration Discrete variables Levels of aquatic vegetation Soil type Water body type 6.7 Mathematical representation Logistic regression: transformation of response variable to get linear relationship logit (log of probability of success/probability of failure) Explanatory variable is in log(odds) Equation for logistic regression: \\[ Log (p/1-p) = b0+b1*x1+e \\] Log (p/1-p): response variable b0: y intercept x: predictor variable b1: slope for explanatory variable 1 e: error If we had more explanatory variables, we could keep adding them to the equation above as b2*x2 and so forth. 6.8 Key binomial logistic model assumptions Dependent variable has 2 levels or is a proportion of successes Observations are independent Normal distribution of data or residuals is not needed 6.9 Creating a logistic model in R 6.9.1 Steps to run a logistic model in R Select potentially interesting predictors Format predictors to correspond to binomial levels Select time period and location Run model Interpret model Plot results 6.9.2 Let’s create this first model together First, we are going to create a new dataset called “filam_diatom” including some potentially interesting variables: filam_diatom &lt;- ham[, c(&quot;area_group&quot;, &quot;season&quot;, &quot;filamentous_Diatom&quot;, &quot;mean_mixing_depth_temp&quot;)] Location Season Filamentous diatom biomass Epilimnion temperat. deep 2 0.0 21.8 deep 2 3.1 21.8 NE 1 NA 19.9 deep 1 NA 19.7 NE 1 NA 13.9 deep 1 287.2 16.1 Let’s consider that filamentous diatom is our response variable of interest, as this food source is hard for zooplankton to consume. We will look at epilimnion temperature as a potential explanatory variable. Before we can get started with the analysis, we need to remove NA data: filam_diatom &lt;- na.omit(filam_diatom) Now we will create a new column to describe presence or absence of filamentous diatoms. We can do that with a function called ifelse to label all observations where measurements were greater than zero as “present”, and all observations where measurements were equal to zero as “absent”: filam_diatom$filam_presence &lt;- ifelse(filam_diatom$filamentous_Diatom &gt; 0, &quot;present&quot;, &quot;absent&quot;) Now we format this column as factor and ensure the reference group (“absent”) is the first to be shown: filam_diatom$filam_presence &lt;- factor(filam_diatom$filam_presence) levels(filam_diatom$filam_presence) [1] &quot;absent&quot; &quot;present&quot; Subsetting to analyse at specific times of the year and at specific locations Here we select summer conditions and remove deep locations: filam_diatom &lt;- filam_diatom[(filam_diatom$season == 2 | filam_diatom$season == 3) &amp; filam_diatom$area_group != &quot;deep&quot;, ] Run model using glm function and family binomial model &lt;- glm(filam_presence ~ mean_mixing_depth_temp, data = filam_diatom, family = binomial) Now we can check the model results summary(model) Call: glm(formula = filam_presence ~ mean_mixing_depth_temp, family = binomial, data = filam_diatom) Deviance Residuals: Min 1Q Median 3Q Max -2.000 -0.897 0.514 0.873 1.238 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 10.158 3.840 2.65 8.2e-03 ** mean_mixing_depth_temp -0.420 0.174 -2.42 1.6e-02 * --- Signif. codes: 0 &#39;***&#39; 1e-03 &#39;**&#39; 1e-02 &#39;*&#39; 5e-02 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 79.807 on 69 degrees of freedom Residual deviance: 72.195 on 68 degrees of freedom AIC: 76.2 Number of Fisher Scoring iterations: 5 So we can see that our p-value for the epilimnion temperature predictor is smaller than 0.05. This suggests that there is a statistically significant relationship between temperature and the presence of filamentous diatoms. Let’s see what each component of the model result summary means: We are now ready for the best part: plotting model predictions First, we calculate predicted probabilities for different temperature values: # Create a sequence of temperature values mean_mixing_depth_temp_values &lt;- seq(min(filam_diatom$mean_mixing_depth_temp), max(filam_diatom$mean_mixing_depth_temp), length.out = length(filam_diatom$mean_mixing_depth_temp)) # Create a data frame with these temperature values newdata &lt;- data.frame(mean_mixing_depth_temp = mean_mixing_depth_temp_values) # Predict probabilities for each temperature value predicted_probabilities_filam &lt;- predict(model, newdata = newdata, type = &quot;response&quot;) # Calculate errors around these predictions (confidence # intervals) z &lt;- qnorm(1 - (1 - 0.95)/2) # 95% confidence level se &lt;- sqrt(predicted_probabilities_filam * (1 - predicted_probabilities_filam)/nrow(filam_diatom)) lower_bound &lt;- predicted_probabilities_filam - z * se upper_bound &lt;- predicted_probabilities_filam + z * se Now we are ready to plot the actual data and the model predictions: # Convert &#39;present&#39; and &#39;absent&#39; to 1 and 0 presence_numeric_filam &lt;- ifelse(filam_diatom$filam_presence == &quot;present&quot;, 1, 0) # Plot the actual data along with predicted probabilities and # confidence intervals plot(mean_mixing_depth_temp_values, predicted_probabilities_filam, type = &quot;l&quot;, main = &quot;Filamentous diatom presence&quot;, xlab = &quot;Epilimnion temperature (°C)&quot;, ylab = &quot;Predicted probability&quot;, cex.axis = 1.5, ylim = c(0, 1), lwd = 2) lines(mean_mixing_depth_temp_values, lower_bound, col = &quot;blue&quot;, lty = 2) lines(mean_mixing_depth_temp_values, upper_bound, col = &quot;blue&quot;, lty = 2) points(filam_diatom$mean_mixing_depth_temp, presence_numeric_filam) The dotted curves are confidence intervals, which show us the range in which we are 95% sure about the location of true values, based on our data 6.9.3 Creating your logistic model Now it’s your turn! Run this next logistic model using nitrite/nitrate as a potential predictor, and filamentous diatoms as the response variable again. Try it now Start by creating a dataset for your analysis. Select a name for your new dataset: your_dataset &lt;- ham[, c(&quot;area_group&quot;, &quot;season&quot;, &quot;filamentous_Diatom&quot;, &quot;NO2_NO3_ECCC1m&quot;)] Formatting Remove NA data your_dataset &lt;- na.omit(your_dataset) Now you can create a new column that describes the presence or the absence of filamentous diatoms across the dataset. Use the function ifelse to label all observations where measurements were greater than zero as “present”, and all observations where measurements were equal to zero as “absent”: your_dataset$new_column_name &lt;- ifelse(your_dataset$filamentous_Diatom &gt; 0, &quot;present&quot;, &quot;absent&quot;) Format this new column as a factor your_dataset$filamentous_Diatom &lt;- factor(your_dataset$filamentous_Diatom) # and check how it looks: levels(your_dataset$filamentous_Diatom) [1] &quot;absent&quot; &quot;present&quot; Run model Use glm function and family binomial your_model_name &lt;- glm(&quot;response variable&quot; ~ &quot;explanatory variable&quot;, data = your_dataset, family = binomial) Check model results using the summary function summary(your_model_name) How well did this predictor do? Plot model predictions Start with obtaining model predictions and errors: # Create a sequence of values for nitrite/nitrate N_values &lt;- seq(min(filam_diatom_P$NO2_NO3_ECCC1m), max(filam_diatom_P$NO2_NO3_ECCC1m), length.out = length(filam_diatom_P$NO2_NO3_ECCC1m)) # Create a data frame with nitrite/nitrate values newdata &lt;- data.frame(NO2_NO3_ECCC1m = N_values) # Predict probabilities for each value of nitrite/nitrate predicted_probabilities_P &lt;- predict(model, newdata = newdata, type = &quot;response&quot;) # Calculate confidence intervals z &lt;- qnorm(1 - (1 - 0.95)/2) # 95% confidence level se &lt;- sqrt(predicted_probabilities_P * (1 - predicted_probabilities_P)/nrow(filam_diatom_P)) lower_bound &lt;- predicted_probabilities_P - z * se upper_bound &lt;- predicted_probabilities_P + z * se Now you are ready to plot the data and the predictions: # Convert &#39;present&#39; and &#39;absent&#39; to 1 and 0 presence_numeric_P &lt;- ifelse(filam_diatom_P$filam_presence == &quot;present&quot;, 1, 0) # Plot the predicted probabilities with confidence intervals plot(N_values, predicted_probabilities_P, type = &quot;l&quot;, main = &quot;Filamentous diatom presence&quot;, xlab = &quot;Nitrate/nitrite (mg/L)&quot;, ylab = &quot;Predicted probability&quot;, cex.axis = 1.5, ylim = c(0, 1), lwd = 2) lines(N_values, lower_bound, col = &quot;blue&quot;, lty = 2) lines(N_values, upper_bound, col = &quot;blue&quot;, lty = 2) points(filam_diatom_P$NO2_NO3_ECCC1m, presence_numeric_P) 6.10 Additional resources 6.10.1 What could go wrong? Mixing up predictors and response variables in the logistic model equation If you get a warning or error when running your logistic model, you may have mixed up the predictors and the response variables. The response variable should be the first one to appear after the opening parenthesis In addition to the variable mix-up, such warning or errors tell us that we may be using quantitative measures as our response variable, which is not appropriate for a logistic regression. See how this error looks like: model &lt;- glm(mean_mixing_depth_temp ~ filam_presence, data = filam_diatom, family = binomial) Error in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1 Incorrectly coding data so that it is not independent Imagine you have counts of living and dead organisms in this imaginary dataset: date location living_daphnia dead_daphnia Jan-1-2024 station-1 40 12 Jan-1-2024 station-2 11 31 Jan-1-2024 station-3 25 0 In this case, instead of considering each individual as “living” or “dead”, you should calculate the proportion of living organisms per replicate like this: example_independence$proportion &lt;- round(example_independence$living_daphnia/(example_independence$living_daphnia + example_independence$dead_daphnia), 2) # the &#39;round&#39; function ensures we have 2 decimals in our # proportion values date location living_daphnia dead_daphnia proportion Jan-1-2024 station-1 40 12 0.77 Jan-1-2024 station-2 11 31 0.26 Jan-1-2024 station-3 25 0 1.00 This proportion will be your response variable for the logistic model. When using proportions, you should also provide the “weights” information in the glm formula (i.e., a dataset with total number of trials per replicate, or the sum of events where we got success + events where we got failure). Categorical response variable is formatted as character instead of factor Formatting data as factors allows for defining order or reference group for statistical testing # Here we are selecting and formatting the binomial data as # we did before, but now we accidentally forget to format the # new column as &#39;factor&#39; example &lt;- ham example$filam_presence &lt;- ifelse(example$filamentous_Diatom &gt; 0, &quot;present&quot;, &quot;absent&quot;) model &lt;- glm(filam_presence ~ mean_mixing_depth_temp, data = example, family = binomial) Error in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1 Check the data in the problematic column: str(example$filam_presence) chr [1:742] &quot;present&quot; NA NA &quot;present&quot; NA NA NA NA NA NA NA NA &quot;present&quot; NA &quot;present&quot; NA NA NA NA NA NA &quot;absent&quot; &quot;present&quot; &quot;present&quot; &quot;absent&quot; &quot;present&quot; NA NA NA &quot;present&quot; &quot;absent&quot; &quot;present&quot; NA NA NA &quot;present&quot; NA NA NA NA NA NA NA NA &quot;present&quot; NA NA &quot;absent&quot; &quot;present&quot; NA &quot;present&quot; &quot;present&quot; &quot;present&quot; NA &quot;present&quot; NA NA NA &quot;present&quot; &quot;present&quot; NA &quot;present&quot; &quot;present&quot; NA &quot;present&quot; NA &quot;absent&quot; &quot;present&quot; &quot;present&quot; &quot;present&quot; &quot;present&quot; &quot;absent&quot; NA &quot;present&quot; &quot;present&quot; &quot;absent&quot; NA NA NA NA NA ... See how we need to remove NA data and format the column as factor for our model to run nicely: example &lt;- example[!is.na(example$filamentous_Diatom), ] example$filam_presence &lt;- factor(example$filam_presence) levels(example$filam_presence) [1] &quot;absent&quot; &quot;present&quot; model &lt;- glm(filam_presence ~ mean_mixing_depth_temp, data = example, family = binomial) No more errors now! That’s it for now, but if you are interested in more complex logistic models, here are some resources: 6.10.2 R resources Multiple logistic regression Building Skills in Quantitative Biology (Cuddington, Edwards, &amp; Ingalls, 2022) Getting started with Multivariate Multiple Regression (Ford, 2024) Multinomial logistic regression Multinomial logistic regression (Russell, 2022) Mixed-effects logistic regression Mixed-Effects Binomial Logistic Regression (Schweinberger, 2022 ) Mixed-effects logistic regression (Sonderegger, Wagner, &amp; Torreira, 2018) "],["multivariate-analysis.html", "7 Multivariate analysis 7.1 Introduction to multivariate analysis 7.2 Principle Component Analysis (PCA) 7.3 Non-metric multidimensional scaling (nMDS) 7.4 Other resources", " 7 Multivariate analysis 7.1 Introduction to multivariate analysis In previous sections, we have discussed scenarios where there is one response variable. If we have multiple responses, \\(y_1\\)…\\(y_n\\), and multiple predictors, \\(x_1\\)…\\(x_n\\), then we need multivariate approaches. These methods allow us to represent the variables or observations in a lower-dimensional space, such as a two-dimensional or three-dimensional plot, while preserving the overall structure of the data. OUTLINE: “Large zooplankton such as Daphnia, large copepods or predatory Cladocera (Bythotrephes, Cercopagis, Leptodora) are much better prey for forage fishes, so changes in their populations (or shifting drivers) are of particular interest.” Question: What are the major drivers of Diaphnia biomass? Variables: Daphnia biomass (mg/m3), water column temperature (°C), epilimnion temperature(°C), particulate organic nitrogen (mg/L), dissolved inorganic carbon (mg/L), particulate organic carbon (mg/L) A good practice before running any analysis is to subset a new dataframe which contains the variables we are interested in. ## Select the variables interested ham.multi &lt;- ham[, c(&quot;area_group&quot;, &quot;season&quot;, &quot;Station_Acronym&quot;, &quot;Daphnia&quot;, &quot;watercolumn_temp&quot;, &quot;mean_mixing_depth_temp&quot;, &quot;PON_ECCC1m&quot;, &quot;DIC_ECCC1m&quot;, &quot;POC_ECCC1m&quot;)] ## Remove all rows containing NAs ham.multi &lt;- na.omit(ham.multi) ## Rename the columns colnames(ham.multi) &lt;- c(&quot;area&quot;, &quot;season&quot;, &quot;station&quot;, &quot;daphnia&quot;, &quot;column.temp&quot;, &quot;epili.temp&quot;, &quot;pon&quot;, &quot;dic&quot;, &quot;poc&quot;) In our new dataframe, different seasons are represented by numeric numbers from 1 to 5. We would like to re-code them into string factors, for easier visualization in later graphics. There are many ways to do this, here we introduce using the factor() function. ## Recode the season numeric code into factors table(ham.multi$season) 1 2 3 4 57 113 118 40 ham.multi$season &lt;- factor(ham.multi$season, levels = c(1, 2, 3, 4), labels = c(&quot;spring&quot;, &quot;early summer&quot;, &quot;late summer&quot;, &quot;early fall&quot;)) 7.2 Principle Component Analysis (PCA) Principle component analysis is a linear transformation method that converts the original set of variables into a new set of linearly uncorrelated variables, called principal components (PCs), which are sorted in decreasing order of variance. 7.2.1 Correlation examination First of all, we need to examine the correlation between our variables. We can achieve this by running a correlation test using cor() function, or creating a correlation plot using pairs() function. ## Correlation table cor.df &lt;- cor(ham.multi[4:9]) knitr::kable(cor.df) daphnia column.temp epili.temp pon dic poc daphnia 1.000 0.229 0.323 0.123 -0.117 0.144 column.temp 0.229 1.000 0.817 0.308 -0.482 0.367 epili.temp 0.323 0.817 1.000 0.310 -0.502 0.386 pon 0.123 0.308 0.310 1.000 -0.283 0.946 dic -0.117 -0.482 -0.502 -0.283 1.000 -0.385 poc 0.144 0.367 0.386 0.946 -0.385 1.000 ## Correlation plot pairs(ham.multi[4:8], main = &quot;Ham Data&quot;, pch = as.numeric(ham.multi$season), col = (ham.multi$season)) # colored depending on season Dimension reduction techniques such PCA works the best when variables are strongly correlated with each other. From the above correlation test output and plot, we can see that some variables clearly have a linear relationship, such as water column temperature and epilimnion temperature, or water column temperature and dissolved inorganic carbon. 7.2.2 PCA with standardized data Now we can start with running our principle component analyses. PCA can be computed using various functions in R, such as prcomp() in stats package, princomp() in stats package, rda() in vegan package. Important Note: The rda() function in vegan allows users to conduct both PCA and RDA, depending on whether the data is constrained or not. When it is unconstrained (like in our example), it is running a PCA! Here we demonstrate using the vegan package, since it also allows easy visualization of our results. Keep in mind that we need to run the PCA on all columns containing continuous variables, which is column 4 - 9 in the ham.multi dataframe. ## Run PCA analysis pca.ham &lt;- rda(ham.multi[, 4:9], scale = TRUE) #subset for all continuous variables After completing the dimension reduction process, each sample now appears as a point in space specified by its new position along the principle component axes. There coordinates are referred to as “site scores” in rda() results, and we can access such information with the scores() function. scores(pca.ham, display = &quot;sites&quot;) Meanwhile, our original variables are projected on the the new principle components. They are defined as “Loadings” and are referred to as “species scores” in rda() results. This information can be obtained with the scores() function too. scores(pca.ham, display = &quot;species&quot;) Remember: “Sites” refer to your observations (the rows in your dataset). “Species” refer to your descriptors (the columns in your dataset), which are the different environmental variables. 7.2.3 Screeplot Now, let’s determine how many principle components to retain for further analysis. The screeplot() function allows us to visualize the variance explained by each of the principle components. Ideally, a curve should be steep, and then bend at an “elbow”, after which the curve flattens out. The first few principle components usually account for a large portion of the variance in the data, and should be retained. ## Screeplot screeplot(pca.ham, type = (&quot;lines&quot;), main = &quot;Screeplot&quot;, pch = 16, cex = 1) Meanwhile, we can also look at the proportional variance explained by each principle component. Such information is available in the summary() of our PCA results. We see the first two PCs together explain roughly 72% of the total variance in this dataset. Along with the screeplot, we are confident that the first two PC are sufficient enough to represent our data. For easier usage in future, let’s store them in a list: pvar &lt;- c(50.22, 21.43) pvar [1] 50.2 21.4 7.2.4 Plot ordination for PCA After we have chosen the first two principle components, now let’s start visualizing our multidimensional data in a 2-dimensional space. As we have seen in previous materials, there are many different methods/packages for creating plots in R. Some of these resources are listed in the “Other Resources” section at the end if you are interested. For this tutorial, we demonstrate using the vegan package to visualize our multivariate results. In general, plotting ordination with vegan follows two steps: Use ordiplot() to create an empty canvas (You can specify the title, axes, limits, and many other features during this step). Use points() to add points representing samples or variables in the new dimensions. Use text() to add labels. Plot 1 - Site Plot (samples) we can visualize the positions of our samples on the new axes (PC1 and PC2). This is the “site plot”. # Create a blank plot ordiplot(pca.ham, type = &quot;n&quot;, main = &quot;Individuals (&#39;sites&#39;)&quot;, xlab = paste0(&quot;PC1 (&quot;, pvar[1], &quot;%)&quot;), ylab = paste0(&quot;PC2 (&quot;, pvar[2], &quot;%)&quot;), ylim = c(-1.2, 1.2)) # Use points() to add points points(pca.ham, display = &quot;sites&quot;, cex = 1) # add sites What if we are interested in the seasonal patterns of our samples on the reduced dimensions? We can use the same graphing techniques, but group our dataset using different seasons. Make sure to label your seasons in the “legend”. ## Create an empty canvas ordiplot(pca.ham, type = &quot;n&quot;, main = &quot;Individuals (&#39;sites&#39;) by season&quot;, xlab = paste0(&quot;PC1 (&quot;, pvar[1], &quot;%)&quot;), ylab = paste0(&quot;PC2 (&quot;, pvar[2], &quot;%)&quot;), ylim = c(-1.2, 1.2)) # specify the y-axis limits ## Use points() to add points points(pca.ham, display = &quot;sites&quot;, pch = as.numeric(ham.multi$season) + 14, col = c(2, 3, 4, 5)[as.numeric(ham.multi$season)], cex = 1) # add sites ## Add legends legend(&quot;bottomright&quot;, legend = unique(ham.multi$season), pch = as.numeric(ham.multi$season) + 14, col = c(2, 3, 4, 5), bty = &quot;n&quot;) # bty specifys legend box boarder There are many other functions in the vegan package that allows customization of your ordination plots. For example, ordiellipse(pca.ham, display=\"sites\", conf=0.95, kind=\"sd\", groups=ham.multi$season) function allows us to add 95% confidence intervals to each group of samples. Try it now: Let’s try creating a sites plot for the same multivariate dataset, but color samples based on different “area_group”. Use colors of your own choice! ## Create an empty canvas ordiplot() ## Use points() to add points and add legends points() legend() Plot 2 - Biplot (samples and variables) Finally, we can visualize both “sites” (individual observations) and “species” (variables) on one graph with a biplot. This time, we use the biplot() function in base R to do this. ## Biplot for PCA biplot(pca.ham, display = c(&quot;sites&quot;, &quot;species&quot;), type = c(&quot;text&quot;, &quot;points&quot;), main = &quot;Biplot&quot;, xlab = paste0(&quot;PC1 (&quot;, pvar[1], &quot;%)&quot;), ylab = paste0(&quot;PC2 (&quot;, pvar[2], &quot;%)&quot;)) Some important points for the biplot() function: biplot() allows different scaling options to preserve either the Euclidean distance (in multidimensional space) among objects (sites), or the correlations/covariances between variables (species). To learn more, please refer to the function description or these online tutorials: QCBS workshop - Unconstrained Ordination Scaling; Tutorial 14.2 - Principle Component Analysis (PCA). biplot() can also be used to show only the samples or variables, by setting display = \"sites\" or display = \"species\". However, it does not allow us to visualize samples in different groups. This is why we introduce the slightly more complicated ordiplot() method in the previous sections. 7.3 Non-metric multidimensional scaling (nMDS) The produced biplot in PCA represents well the distance among objects, but fails to represent the whole variation dimensions of the ordination space. Unlike PCA, non-metric multidimensional scaling (nMDS) does not to preserve the exact dissimilarities (distance) among objects in an ordination plot, instead it represents as well as possible the ordering relationships among objects in a small and specified number of axes. In other words, the goal of nMDS is to represent the original position of samples in multidimensional space as accurately as possible using a reduced number of dimensions. We can use the metaMDS() function in the vegan package to conduct non-metric multidimensional scaling in R. In addition to the input dataframe, this function also requires us to specify the distance measure distance = ? and number of reduced dimensions k = ?. ## Run nMDS nmds.ham &lt;- metaMDS(ham.multi[, 4:9], distance = &quot;bray&quot;, k = 2, trace = FALSE) ## Stress nmds.ham$stress [1] 0.136 From the nMDS results, we can extract the stress parameter. Stress identifies how well points fit within the specified number of dimensions. A good rule of thumb for stress: \\(&gt;0.2\\) Poor (risk in interpretation) \\(0.1-0.2\\) Fair (some distances misleading) \\(0.05-0.1\\) Good (inferences confident) \\(&lt;0.05\\) Excellent representation 7.3.1 Shepard Plot We can use a Shepard plot to learn about the distortion of representation. On the x-axis, it plots the original dissimilarity (original distances in full dimensions). On the y-axis, it plots the distances in the reduced dimensional space. Ideally, a really accurate dimension reduction will produce a straight line. ## Shepard plot stressplot(nmds.ham, pch = 16, las = 1, main = &quot;Shepard plot&quot;) 7.3.2 Plot ordination for nMDS No we can plot the ordination for nMDS, just like for PCA in the previous sections. The steps exactly the same. We use ordiplot() to create an empty canvas first, then use points() to add samples/variables. Additionally, we would like to represent species scores with arrows. This step can only be done manually by extracted the species scores using scores(display = \"species\"), and draw arrows using arrows() function. ## Create empty canvas ordiplot(nmds.ham, type = &quot;n&quot;, main = &quot;nMDS Ordination&quot;) ## Add points and legends points(nmds.ham, display = &quot;sites&quot;, pch = as.numeric(ham.multi$season) + 14, col = c(2, 3, 4, 5)[as.numeric(ham.multi$season)], cex = 1) # add sites legend(&quot;bottomright&quot;, legend = unique(ham.multi$season), pch = as.numeric(unique(ham.multi$season)) + 14, col = c(2, 3, 4, 5), bty = &quot;n&quot;, cex = 1) # add legend ## Add species scores manually species_scores &lt;- scores(nmds.ham, display = &quot;species&quot;) arrows(0, 0, species_scores[, 1], species_scores[, 2], col = &quot;black&quot;, length = 0.1) # draw arrow from (0,0) text(species_scores[, 1], species_scores[, 2], labels = rownames(species_scores), col = &quot;black&quot;, pos = c(2, 2, 3, 4, 2, 3)) 7.4 Other resources Multivariate analyses tutorials Building Skills in Quantitative Biology QCBS R Workshop Series - Multivariate Analyses in R Running NMDS using ‘metaMDS’(nMDS tutorial with vegan package) Introduction to Ordination Useful R packages factoextra (for visualizing PCA results) learnPCA (an R package for PCA learning) ggbiplot "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
